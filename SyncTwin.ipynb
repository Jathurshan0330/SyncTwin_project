{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99ddaf2a-fe69-4785-b9a0-9c4232abf07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Trying to reproduce the results using transformers\n",
    "\n",
    "import copy\n",
    "from typing import Optional, Any\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Module\n",
    "from torch.nn import MultiheadAttention\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import Linear\n",
    "from torch.nn import LayerNorm, BatchNorm1d\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from config.config import DEVICE\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6e1b9-7a69-4b11-9af9-e2e00eef8a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d88fb9f-59d2-4a4c-bda8-9f71c67137a2",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7494cc5-1591-4d75-85c2-bb7df3746797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_config(data_path, fold=\"train\"):\n",
    "    with open(data_path.format(fold, \"config\", \"pkl\"), \"rb\") as f:\n",
    "        config = pickle.load(file=f)\n",
    "    n_units = config[\"n_units\"]\n",
    "    n_treated = config[\"n_treated\"]\n",
    "    n_units_total = config[\"n_units_total\"]\n",
    "    step = config[\"step\"]\n",
    "    train_step = config[\"train_step\"]\n",
    "    control_sample = config[\"control_sample\"]\n",
    "    noise = config[\"noise\"]\n",
    "    n_basis = config[\"n_basis\"]\n",
    "    n_cluster = config[\"n_cluster\"]\n",
    "    return n_units, n_treated, n_units_total, step, train_step, control_sample, noise, n_basis, n_cluster\n",
    "\n",
    "def load_tensor(data_path, fold=\"train\"):\n",
    "    print(data_path.format(fold, \"x_full\", \"pth\"))\n",
    "    x_full = torch.load(data_path.format(fold, \"x_full\", \"pth\"))\n",
    "    t_full = torch.load(data_path.format(fold, \"t_full\", \"pth\"))\n",
    "    mask_full = torch.load(data_path.format(fold, \"mask_full\", \"pth\"))\n",
    "    batch_ind_full = torch.load(data_path.format(fold, \"batch_ind_full\", \"pth\"))\n",
    "    y_full = torch.load(data_path.format(fold, \"y_full\", \"pth\"))\n",
    "    y_control = torch.load(data_path.format(fold, \"y_control\", \"pth\"))\n",
    "    y_mask_full = torch.load(data_path.format(fold, \"y_mask_full\", \"pth\"))\n",
    "    m = torch.load(data_path.format(fold, \"m\", \"pth\"))\n",
    "    sd = torch.load(data_path.format(fold, \"sd\", \"pth\"))\n",
    "    treatment_effect = torch.load(data_path.format(fold, \"treatment_effect\", \"pth\"))\n",
    "    return x_full, t_full, mask_full, batch_ind_full, y_full, y_control, y_mask_full, m, sd, treatment_effect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63dec31e-0d49-4782-97aa-ea97a08ea7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/sync6d-p10-seed-100\"+ \"/{}-{}.{}\"\n",
    "n_units, n_treated, n_units_total, step, train_step, control_sample, noise, n_basis, n_cluster = load_config(\n",
    "    data_path, \"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eb9fa60-bac1-4a0f-9ba0-44938f101e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 200 1200 30 25 1000 0.1 6 2\n"
     ]
    }
   ],
   "source": [
    "print(n_units, n_treated, n_units_total, step, train_step, control_sample, noise, n_basis, n_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a5eca5a-4f13-430d-a400-2bd71479c0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/sync6d-p10-seed-100/train-x_full.pth\n"
     ]
    }
   ],
   "source": [
    "(x_full,t_full,mask_full,batch_ind_full,y_full,y_control,y_mask_full,m,sd,treatment_effect,) = load_tensor(data_path, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483279f3-d5a6-4274-a50f-b074e73d1c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_full: torch.Size([25, 1200, 3])\n",
      "t_full: torch.Size([25, 1200, 3])\n",
      "mask_full: torch.Size([25, 1200, 3])\n",
      "batch_ind_full: torch.Size([1200])\n",
      "y_full: torch.Size([5, 1200, 1])\n",
      "y_control: torch.Size([5, 1000, 1])\n",
      "treatment_effect: torch.Size([5, 200, 1])\n",
      "y_mask_full: torch.Size([1200])\n",
      "m: torch.Size([3]) tensor([0.0118, 0.1373, 1.0291], device='cuda:3')\n",
      "sd: torch.Size([3]) tensor([0.7947, 3.5020, 7.6243], device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "print(f'x_full: {x_full.shape}') ### Temporal Covariates\n",
    "print(f't_full: {t_full.shape}') ###  Time -25 to 4\n",
    "print(f'mask_full: {mask_full.shape}') ### Masking vector\n",
    "print(f'batch_ind_full: {batch_ind_full.shape}') ### Batch indexes\n",
    "print(f'y_full: {y_full.shape}')   ### y_i ### need to predict this\n",
    "print(f'y_control: {y_control.shape}') #### y_i(0)\n",
    "print(f'treatment_effect: {treatment_effect.shape}')  #### y_i(1)\n",
    "print(f'y_mask_full: {y_mask_full.shape}') ### To separate control and treatment groups  \n",
    "print(f'm: {m.shape} {m}') \n",
    "print(f'sd: {sd.shape} {sd}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c8d5a04-7a0c-4b20-a391-21ddfebc984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDL_Stim_Dataset(Dataset):\n",
    "    def __init__(self, data_path, fold,device):\n",
    "        # Get the data\n",
    "        (self.x_full,self.t_full,self.mask_full,self.batch_ind_full,\n",
    "         self.y_full,self.y_control,self.y_mask_full,\n",
    "         self.m,self.sd,self.treatment_effect,) = load_tensor(data_path, fold)\n",
    "        print(self.m,self.sd)\n",
    "        # print(self.x_full.max(),self.x_full.min(),self.x_full.mean())\n",
    "        self.x_full = torch.moveaxis(self.x_full,1,0)\n",
    "        # self.x_full = torch.moveaxis(self.x_full,1,-1)\n",
    "        self.t_full = torch.moveaxis(self.t_full,1,0)\n",
    "        # self.t_full = torch.moveaxis(self.t_full,1,-1)\n",
    "        self.mask_full = torch.moveaxis(self.mask_full,1,0)\n",
    "        # self.mask_full = torch.moveaxis(self.mask_full,1,-1)\n",
    "        self.y_full = torch.moveaxis(self.y_full,1,0).squeeze()\n",
    "        self.y_control = torch.moveaxis(self.y_control,1,0)\n",
    "        self.treatment_effect = torch.moveaxis(self.treatment_effect,1,0)\n",
    "        \n",
    "        self.device = device\n",
    "        # for i in range (self.x_full.shape[-1]):\n",
    "        #     self.x_full[:,:,i] = (self.x_full[:,:,i] - self.m[i])/self.sd[i]\n",
    "        # print(self.x_full.max(),self.x_full.min(),self.x_full.mean())\n",
    "        print(f'x_full: {self.x_full.shape}') ### Temporal Covariates\n",
    "        print(f't_full: {self.t_full.shape}') ###  Time -25 to 4\n",
    "        print(f'mask_full: {self.mask_full.shape}') ### Masking vector\n",
    "        print(f'batch_ind_full: {self.batch_ind_full.shape}') ### Batch indexes\n",
    "        print(f'y_full: {self.y_full.shape}')   ### y_i ### need to predict this\n",
    "        print(f'y_control: {self.y_control.shape}') #### y_i(0)\n",
    "        print(f'treatment_effect: {self.treatment_effect.shape}')  #### y_i(1)\n",
    "        print(f'y_mask_full: {self.y_mask_full.shape}') ### if outcome not available during \n",
    "        print(f'm: {self.m.shape}') \n",
    "        print(f'sd: {self.sd.shape}')\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_full)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x_full[idx].to(self.device)    \n",
    "        t = self.t_full[idx].to(self.device)    \n",
    "        m = self.mask_full[idx].to(self.device)    \n",
    "        y = self.y_full[idx].to(self.device)    \n",
    "        y_mask = self.y_mask_full[idx].unsqueeze(-1).to(self.device)    \n",
    "        return x,t,m,y,y_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdd52b89-d1d2-4e61-a9a7-0eb845ed7419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/sync6d-p10-seed-100/train-x_full.pth\n",
      "tensor([0.0118, 0.1373, 1.0291], device='cuda:3') tensor([0.7947, 3.5020, 7.6243], device='cuda:3')\n",
      "x_full: torch.Size([1200, 25, 3])\n",
      "t_full: torch.Size([1200, 25, 3])\n",
      "mask_full: torch.Size([1200, 25, 3])\n",
      "batch_ind_full: torch.Size([1200])\n",
      "y_full: torch.Size([1200, 5])\n",
      "y_control: torch.Size([1000, 5, 1])\n",
      "treatment_effect: torch.Size([200, 5, 1])\n",
      "y_mask_full: torch.Size([1200])\n",
      "m: torch.Size([3])\n",
      "sd: torch.Size([3])\n",
      "./data/sync6d-p10-seed-100/val-x_full.pth\n",
      "tensor([0.0024, 0.0946, 0.9151], device='cuda:3') tensor([0.7631, 3.3575, 7.2832], device='cuda:3')\n",
      "x_full: torch.Size([1200, 25, 3])\n",
      "t_full: torch.Size([1200, 25, 3])\n",
      "mask_full: torch.Size([1200, 25, 3])\n",
      "batch_ind_full: torch.Size([1200])\n",
      "y_full: torch.Size([1200, 5])\n",
      "y_control: torch.Size([1000, 5, 1])\n",
      "treatment_effect: torch.Size([200, 5, 1])\n",
      "y_mask_full: torch.Size([1200])\n",
      "m: torch.Size([3])\n",
      "sd: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./data/sync6d-p10-seed-100\"+ \"/{}-{}.{}\"\n",
    "train_dataset = LDL_Stim_Dataset(data_path, fold = 'train',device = DEVICE)\n",
    "val_dataset = LDL_Stim_Dataset(data_path, fold = 'val',device = DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a5c700e-b477-4817-a40b-79d4ca443010",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a41e1ef0-4e3d-4f73-9d7f-b696b6d8cc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 25, 3]) torch.Size([32, 25, 3]) torch.Size([32, 25, 3]) torch.Size([32, 5]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "x,t,m,y,y_mask   = next(iter(train_data_loader))\n",
    "print(x.shape,t.shape,m.shape,y.shape,y_mask.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4257d08-85f3-4f72-86dd-6a3fb78116c0",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70b5ba82-c99d-41d1-b8ab-3ed1f404bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "\n",
    "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5330de0-83e6-4c5e-93a0-8ae4f98e473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_Embedding(nn.Module): \n",
    "    def __init__(self, in_channels: int = 3, emb_size: int = 64):\n",
    "        super(get_Embedding, self).__init__()\n",
    "\n",
    "        self.projection =  nn.Sequential(\n",
    "            Rearrange('b s e -> b e s'),\n",
    "            nn.Conv1d(in_channels, emb_size, kernel_size = 1, stride = 1),\n",
    "            # Rearrange('b e s -> b s e')\n",
    "            )\n",
    "            \n",
    "        self.arrange1 = Rearrange('b e s -> s b e')\n",
    "        self.pos = PositionalEncoding(d_model=emb_size)\n",
    "        self.arrange2 = Rearrange('s b e -> b s e  ')\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.projection(x)  \n",
    "        # add position embedding\n",
    "        x = self.arrange1(x)\n",
    "        x = self.pos(x)\n",
    "        x = self.arrange2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d396614-3ab8-4c47-8faa-44469a50f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atten_block(nn.Module): \n",
    "    def __init__(self, d_model=64, nhead=8, dropout=0.1,dim_feedforward=512,\n",
    "                 layer_norm_eps=1e-5):\n",
    "        super(Atten_block, self).__init__()\n",
    "       \n",
    "        self.norm = LayerNorm(d_model, eps=layer_norm_eps)#, **factory_kwargs)  \n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)#,\n",
    "                                            # **factory_kwargs)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "        \n",
    "        self.norm_ff = LayerNorm(d_model, eps=layer_norm_eps)#, **factory_kwargs)\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)#, **factory_kwargs)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)#, **factory_kwargs)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    " \n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        src = x\n",
    "        src2 = self.self_attn(src, src, src)[0]\n",
    "        out = src + self.dropout(src2)\n",
    "        out = self.norm(out)   ########\n",
    "        \n",
    "        src2 = self.linear2(self.dropout1(self.relu(self.linear1(out))))\n",
    "        out = out + self.dropout2(src2)\n",
    "        out = self.norm_ff(out)\n",
    "        return out                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "285a8595-f745-42c3-9728-c669b54305ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trans_Encoder(nn.Module): \n",
    "    def __init__(self, in_channels = 3,d_model=64, nhead=8, dropout=0.1,dim_feedforward=512,\n",
    "                 layer_norm_eps=1e-5):\n",
    "        super(Trans_Encoder, self).__init__()\n",
    "        \n",
    "        self.embed = get_Embedding(in_channels = in_channels, emb_size = d_model)\n",
    "        self.atten_1 =  Atten_block(d_model=d_model, nhead=nhead, dropout=dropout,\n",
    "                                    dim_feedforward=dim_feedforward,layer_norm_eps=layer_norm_eps)\n",
    "        self.atten_2 =  Atten_block(d_model=d_model, nhead=nhead, dropout=dropout,\n",
    "                                    dim_feedforward=dim_feedforward,layer_norm_eps=layer_norm_eps)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.atten_1(x)\n",
    "        x = self.atten_2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class Trans_Decoder(nn.Module): \n",
    "    def __init__(self,out_channels= 3, d_model=64, nhead=8, dropout=0.1,dim_feedforward=512,\n",
    "                 layer_norm_eps=1e-5):\n",
    "        super(Trans_Decoder, self).__init__()\n",
    "        \n",
    "        # self.embed = get_Embedding(in_channels = 3, emb_size = d_model)\n",
    "        self.atten_1 =  Atten_block(d_model=d_model, nhead=nhead, dropout=dropout,\n",
    "                                    dim_feedforward=dim_feedforward,layer_norm_eps=layer_norm_eps)\n",
    "        self.atten_2 =  Atten_block(d_model=d_model, nhead=nhead, dropout=dropout,\n",
    "                                    dim_feedforward=dim_feedforward,layer_norm_eps=layer_norm_eps)\n",
    "        self.final = Linear(d_model, out_channels)\n",
    "        # self.arrange1 = Rearrange('b e s -> s b e')\n",
    "    def forward(self, c):\n",
    "        # x = self.embed(x)\n",
    "        x = self.atten_1(c)\n",
    "        x = self.atten_2(x)\n",
    "        x = self.final(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "\n",
    "class linear_cls(nn.Module):\n",
    "    def __init__(self,y_times = 5):\n",
    "        super(linear_cls, self).__init__()\n",
    "        \n",
    "        self.Q = nn.Linear(1600, y_times)\n",
    "    def forward(self, c):\n",
    "        x = torch.flatten(c, start_dim=1, end_dim=- 1) \n",
    "        x = self.Q(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97e16223-b08f-4c46-aa79-267848d36746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 25, 3]) torch.Size([32, 25, 3]) torch.Size([32, 25, 3]) torch.Size([32, 5]) torch.Size([32, 1])\n",
      "torch.Size([32, 25, 64]) torch.Size([32, 25, 3]) torch.Size([32, 5])\n"
     ]
    }
   ],
   "source": [
    "encoder = Trans_Encoder().to(DEVICE)\n",
    "decoder = Trans_Decoder().to(DEVICE)\n",
    "Q = linear_cls().to(DEVICE)\n",
    "\n",
    "## Testing\n",
    "x,t,m,y,y_mask   = next(iter(train_data_loader))\n",
    "print(x.shape,t.shape,m.shape,y.shape,y_mask.shape )\n",
    "\n",
    "c_hat = encoder(x)\n",
    "x_hat = decoder(c_hat)\n",
    "y_hat = Q(c_hat)\n",
    "\n",
    "print(c_hat.shape,x_hat.shape,y_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679430db-a882-4ed3-89eb-e9ea62ed10a6",
   "metadata": {},
   "source": [
    "## Representation learning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a701babd-fb77-4e51-9882-a4462fbba530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training parameters\n",
    "criterion = nn.L1Loss()\n",
    "lr_u  = 0.001   \n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "n_epochs = 1000\n",
    "save_model_freq = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0aa742a-eb56-49ef-a46c-ca720dbd034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate optimizers\n",
    "opt =  torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters())+ list(Q.parameters()), lr=lr_u, betas=(beta_1, beta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3e35bb3-6230-40a9-af04-2527f339d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_loss(x_hat,x,m):\n",
    "    criterion = nn.MSELoss()\n",
    "    return criterion(x_hat*m,x*m)\n",
    "\n",
    "def sup_loss(y_hat,y,y_mask):\n",
    "    criterion = nn.MSELoss()\n",
    "    return criterion(y_hat*y_mask,y*y_mask)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count if self.count != 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc231d79-ffc2-43a4-8fe4-2b1b3493dd63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1h723bxo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>█▃█▃▂▁▁▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁</td></tr><tr><td>epoch_saved_Best_Model</td><td>▁▁▁▁▂▂▃▄▆▇██</td></tr><tr><td>train_epoch_loss</td><td>█▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training time/Iter</td><td>▄▄▄▄▄▄▃▄▂▃▂▄▃▄▃▅▄▂▃▅█▂▄▄▅▃▃▁▅▄▄▄▄▅▅▂▅▄▄▃</td></tr><tr><td>val_epoch_loss</td><td>█▄▄▃▅▃▂▃▃▄▃▂▂▂▄▃▃▁▂▃▃▂▂▂▂▁▂▄▂▁▂▂▁▃▁▃▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.02233</td></tr><tr><td>epoch_saved_Best_Model</td><td>43</td></tr><tr><td>train_epoch_loss</td><td>0.03004</td></tr><tr><td>training time/Iter</td><td>0.01738</td></tr><tr><td>val_epoch_loss</td><td>0.02532</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fortuitous-dumpling-2</strong>: <a href=\"https://wandb.ai/jathurshan_0330/SyncTwin/runs/1h723bxo\" target=\"_blank\">https://wandb.ai/jathurshan_0330/SyncTwin/runs/1h723bxo</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230204_225358-1h723bxo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1h723bxo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter-jathurshan/SyncTwin_project/wandb/run-20230204_225523-z35375u6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jathurshan_0330/SyncTwin/runs/z35375u6\" target=\"_blank\">golden-wonton-3</a></strong> to <a href=\"https://wandb.ai/jathurshan_0330/SyncTwin\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "is_wandb = True\n",
    "if is_wandb:\n",
    "    import wandb\n",
    "    wandb.init(project=\"SyncTwin\", entity=\"jathurshan_0330\")\n",
    "    wandb.run.name = \"Initial_With_Transformers\"\n",
    "    wandb.run.save()\n",
    "    \n",
    "    exp_path = f\"/home/jupyter-jathurshan/SyncTwin_Results/{wandb.run.name}\"\n",
    "    if not os.path.exists(exp_path):\n",
    "        os.mkdir(exp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e11545a-b005-4646-884c-65dd09a45f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder,decoder,Q,opt,data_loader,is_wandb,verbose_freq = 500):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    Q.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    train_losses = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    for batch_idx, (x,t,m,y,y_mask ) in enumerate(data_loader): \n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        c_hat = encoder(x)\n",
    "        x_hat = decoder(c_hat)\n",
    "        y_hat = Q(c_hat)\n",
    "        \n",
    "        loss = recon_loss(x_hat,x,m) + sup_loss(y_hat,y,y_mask)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        train_losses.update(loss.data.item())\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if (batch_idx+1) % verbose_freq == 0:\n",
    "            msg = 'Epoch: [{0}/{3}][{1}/{2}]\\t' \\\n",
    "                  'Time {batch_time.val:.3f}s ({batch_time.avg:.3f}s)\\t' \\\n",
    "                  'Speed {speed:.1f} samples/s\\t' \\\n",
    "                  'Data {data_time.val:.3f}s ({data_time.avg:.3f}s)\\t' \\\n",
    "                  'Loss {train_loss.val:.5f} ({train_loss.avg:.5f})\\t'.format(\n",
    "                      epoch_idx+1, batch_idx,len(data_loader), n_epochs , batch_time=batch_time,\n",
    "                      speed=x.size(0)/batch_time.val,\n",
    "                      data_time=data_time, train_loss=train_losses)\n",
    "            print(msg)\n",
    "        \n",
    "        if is_wandb:\n",
    "            wandb.log({\"batch_loss\": loss.data.item()})\n",
    "    \n",
    "    \n",
    "    if is_wandb:\n",
    "            wandb.log({\"train_epoch_loss\": train_losses.avg})\n",
    "            wandb.log({\"training time/Iter\": batch_time.sum/len(data_loader)})\n",
    "    \n",
    "    \n",
    "    # print(f\"Evaluation   Epoch : {epoch_idx+1}  =====================>\")\n",
    "    print(f\"Training Epoch Loss: {train_losses.avg}\")#,Training Time/Epoch: {batch_time.sum},Training Time/Iter: {batch_time.sum/len(data_loader)}\")\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d8c56ca-c973-4290-a253-baa6caaf6502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(encoder,decoder,Q,data_loader,is_wandb):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    Q.eval()\n",
    "    \n",
    "    val_losses = AverageMeter()\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x,t,m,y,y_mask) in enumerate(data_loader): \n",
    "            c_hat = encoder(x)\n",
    "            x_hat = decoder(c_hat)\n",
    "            y_hat = Q(c_hat)\n",
    "\n",
    "            loss = recon_loss(x_hat,x,m) + sup_loss(y_hat,y,y_mask)\n",
    "            \n",
    "            val_losses.update(loss.data.item())\n",
    "    \n",
    "    if is_wandb:\n",
    "            wandb.log({\"val_epoch_loss\": val_losses.avg})\n",
    "    \n",
    "    \n",
    "    # print(f\"Evaluation   Epoch : {epoch_idx+1}  =====================>\")\n",
    "    print(f\"Val Epoch Loss: {val_losses.avg}\")\n",
    "    return val_losses.avg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6d9cada-09c6-4bb7-a24d-a376576ecc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch : [1/1000]===============================================================================\n",
      "Training Epoch Loss: 0.12415168429479788\n",
      "Val Epoch Loss: 0.027342624129041246\n",
      "Saving Best Model =======================================>\n",
      "Training Epoch : [2/1000]===============================================================================\n",
      "Training Epoch Loss: 0.026241248571559003\n",
      "Val Epoch Loss: 0.01870288855494245\n",
      "Saving Best Model =======================================>\n",
      "Training Epoch : [3/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02542341461307124\n",
      "Val Epoch Loss: 0.02399390601356955\n",
      "Training Epoch : [4/1000]===============================================================================\n",
      "Training Epoch Loss: 0.029767976681652823\n",
      "Val Epoch Loss: 0.020528076216578484\n",
      "Training Epoch : [5/1000]===============================================================================\n",
      "Training Epoch Loss: 0.027518457193907938\n",
      "Val Epoch Loss: 0.027502896570551553\n",
      "Training Epoch : [6/1000]===============================================================================\n",
      "Training Epoch Loss: 0.024940010258241704\n",
      "Val Epoch Loss: 0.019236070949486213\n",
      "Training Epoch : [7/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02731839194893837\n",
      "Val Epoch Loss: 0.02328476949447864\n",
      "Training Epoch : [8/1000]===============================================================================\n",
      "Training Epoch Loss: 0.04135981211928945\n",
      "Val Epoch Loss: 0.07173599403253511\n",
      "Training Epoch : [9/1000]===============================================================================\n",
      "Training Epoch Loss: 0.03643331044402562\n",
      "Val Epoch Loss: 0.019547809062427597\n",
      "Training Epoch : [10/1000]===============================================================================\n",
      "Training Epoch Loss: 0.024752125634174598\n",
      "Val Epoch Loss: 0.022400729676806612\n",
      "Training Epoch : [11/1000]===============================================================================\n",
      "Training Epoch Loss: 0.025493211701120202\n",
      "Val Epoch Loss: 0.023311143924825285\n",
      "Training Epoch : [12/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02480061401269938\n",
      "Val Epoch Loss: 0.019063178866513465\n",
      "Training Epoch : [13/1000]===============================================================================\n",
      "Training Epoch Loss: 0.028491216319564142\n",
      "Val Epoch Loss: 0.02247102426266984\n",
      "Training Epoch : [14/1000]===============================================================================\n",
      "Training Epoch Loss: 0.03453955064086538\n",
      "Val Epoch Loss: 0.03961003260490926\n",
      "Training Epoch : [15/1000]===============================================================================\n",
      "Training Epoch Loss: 0.030268603935837746\n",
      "Val Epoch Loss: 0.025671567400231174\n",
      "Training Epoch : [16/1000]===============================================================================\n",
      "Training Epoch Loss: 0.028777930983587316\n",
      "Val Epoch Loss: 0.027849260938206787\n",
      "Training Epoch : [17/1000]===============================================================================\n",
      "Training Epoch Loss: 0.03284277129722269\n",
      "Val Epoch Loss: 0.033383438768061366\n",
      "Training Epoch : [18/1000]===============================================================================\n",
      "Training Epoch Loss: 0.031394293727843386\n",
      "Val Epoch Loss: 0.025642711840766042\n",
      "Training Epoch : [19/1000]===============================================================================\n",
      "Training Epoch Loss: 0.026380488403925772\n",
      "Val Epoch Loss: 0.0269000314445676\n",
      "Training Epoch : [20/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02462073229253292\n",
      "Val Epoch Loss: 0.021714031426726205\n",
      "Training Epoch : [21/1000]===============================================================================\n",
      "Training Epoch Loss: 0.023482695516002804\n",
      "Val Epoch Loss: 0.019008265085224258\n",
      "Training Epoch : [22/1000]===============================================================================\n",
      "Training Epoch Loss: 0.027574765005786168\n",
      "Val Epoch Loss: 0.02367724676763541\n",
      "Training Epoch : [23/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02607338998074594\n",
      "Val Epoch Loss: 0.02771790676113022\n",
      "Training Epoch : [24/1000]===============================================================================\n",
      "Training Epoch Loss: 0.027865080898137468\n",
      "Val Epoch Loss: 0.02397455582640281\n",
      "Training Epoch : [25/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02844911034366018\n",
      "Val Epoch Loss: 0.026004797509430272\n",
      "Training Epoch : [26/1000]===============================================================================\n",
      "Training Epoch Loss: 0.023973892638949973\n",
      "Val Epoch Loss: 0.021840434496928202\n",
      "Training Epoch : [27/1000]===============================================================================\n",
      "Training Epoch Loss: 0.026430509788425344\n",
      "Val Epoch Loss: 0.02930703407496606\n",
      "Training Epoch : [28/1000]===============================================================================\n",
      "Training Epoch Loss: 0.026039281026705316\n",
      "Val Epoch Loss: 0.01793288696851385\n",
      "Saving Best Model =======================================>\n",
      "Training Epoch : [29/1000]===============================================================================\n",
      "Training Epoch Loss: 0.0253798763120645\n",
      "Val Epoch Loss: 0.028145117832249718\n",
      "Training Epoch : [30/1000]===============================================================================\n",
      "Training Epoch Loss: 0.029582093004137278\n",
      "Val Epoch Loss: 0.02935461299248824\n",
      "Training Epoch : [31/1000]===============================================================================\n",
      "Training Epoch Loss: 0.021175199629444825\n",
      "Val Epoch Loss: 0.017682249333072258\n",
      "Saving Best Model =======================================>\n",
      "Training Epoch : [32/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02750618618569876\n",
      "Val Epoch Loss: 0.023562015009749877\n",
      "Training Epoch : [33/1000]===============================================================================\n",
      "Training Epoch Loss: 0.021302218499936555\n",
      "Val Epoch Loss: 0.020589792414715414\n",
      "Training Epoch : [34/1000]===============================================================================\n",
      "Training Epoch Loss: 0.019972977424530608\n",
      "Val Epoch Loss: 0.01965985463098868\n",
      "Training Epoch : [35/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02348709787781301\n",
      "Val Epoch Loss: 0.01950654851909923\n",
      "Training Epoch : [36/1000]===============================================================================\n",
      "Training Epoch Loss: 0.024354957387243446\n",
      "Val Epoch Loss: 0.0187683656051951\n",
      "Training Epoch : [37/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02568618458156523\n",
      "Val Epoch Loss: 0.03694795774246909\n",
      "Training Epoch : [38/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02473968899759807\n",
      "Val Epoch Loss: 0.025499787892362003\n",
      "Training Epoch : [39/1000]===============================================================================\n",
      "Training Epoch Loss: 0.027502442514033693\n",
      "Val Epoch Loss: 0.016948487260378897\n",
      "Saving Best Model =======================================>\n",
      "Training Epoch : [40/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02255255652983722\n",
      "Val Epoch Loss: 0.021569481542284944\n",
      "Training Epoch : [41/1000]===============================================================================\n",
      "Training Epoch Loss: 0.022915070533360306\n",
      "Val Epoch Loss: 0.019029192485552477\n",
      "Training Epoch : [42/1000]===============================================================================\n",
      "Training Epoch Loss: 0.020656495011950795\n",
      "Val Epoch Loss: 0.024116246127768567\n",
      "Training Epoch : [43/1000]===============================================================================\n",
      "Training Epoch Loss: 0.022901132261674655\n",
      "Val Epoch Loss: 0.028024643708608653\n",
      "Training Epoch : [44/1000]===============================================================================\n",
      "Training Epoch Loss: 0.021819510742237692\n",
      "Val Epoch Loss: 0.016296687799407857\n",
      "Saving Best Model =======================================>\n",
      "Training Epoch : [45/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02031388834707047\n",
      "Val Epoch Loss: 0.025177170026214106\n",
      "Training Epoch : [46/1000]===============================================================================\n",
      "Training Epoch Loss: 0.024407874057559592\n",
      "Val Epoch Loss: 0.01907012792990396\n",
      "Training Epoch : [47/1000]===============================================================================\n",
      "Training Epoch Loss: 0.021733441978300874\n",
      "Val Epoch Loss: 0.01991646549370336\n",
      "Training Epoch : [48/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02282350194199305\n",
      "Val Epoch Loss: 0.014374450192247567\n",
      "Saving Best Model =======================================>\n",
      "Training Epoch : [49/1000]===============================================================================\n",
      "Training Epoch Loss: 0.025701057729556373\n",
      "Val Epoch Loss: 0.02344726728226401\n",
      "Training Epoch : [50/1000]===============================================================================\n",
      "Training Epoch Loss: 0.019934667543949264\n",
      "Val Epoch Loss: 0.016083832419673472\n",
      "Training Epoch : [51/1000]===============================================================================\n",
      "Training Epoch Loss: 0.021516169887036085\n",
      "Val Epoch Loss: 0.021216286885503092\n",
      "Training Epoch : [52/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01915152885607983\n",
      "Val Epoch Loss: 0.017330272380556715\n",
      "Training Epoch : [53/1000]===============================================================================\n",
      "Training Epoch Loss: 0.017849285474145098\n",
      "Val Epoch Loss: 0.022924194700623814\n",
      "Training Epoch : [54/1000]===============================================================================\n",
      "Training Epoch Loss: 0.0195563484571482\n",
      "Val Epoch Loss: 0.018507280509526793\n",
      "Training Epoch : [55/1000]===============================================================================\n",
      "Training Epoch Loss: 0.019142908852939542\n",
      "Val Epoch Loss: 0.016046828817036982\n",
      "Training Epoch : [56/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01949484935520511\n",
      "Val Epoch Loss: 0.028808950961224343\n",
      "Training Epoch : [57/1000]===============================================================================\n",
      "Training Epoch Loss: 0.025637628951747166\n",
      "Val Epoch Loss: 0.026087015946885866\n",
      "Training Epoch : [58/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02645489735234725\n",
      "Val Epoch Loss: 0.038511607979767416\n",
      "Training Epoch : [59/1000]===============================================================================\n",
      "Training Epoch Loss: 0.03217813990225917\n",
      "Val Epoch Loss: 0.026282171853572914\n",
      "Training Epoch : [60/1000]===============================================================================\n",
      "Training Epoch Loss: 0.024656617964961026\n",
      "Val Epoch Loss: 0.03026757163828925\n",
      "Training Epoch : [61/1000]===============================================================================\n",
      "Training Epoch Loss: 0.022695724852383137\n",
      "Val Epoch Loss: 0.02828330791702396\n",
      "Training Epoch : [62/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02018983961132012\n",
      "Val Epoch Loss: 0.014115603622294179\n",
      "Saving Best Model =======================================>\n",
      "Training Epoch : [63/1000]===============================================================================\n",
      "Training Epoch Loss: 0.023413069570731176\n",
      "Val Epoch Loss: 0.018907143617980182\n",
      "Training Epoch : [64/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02123147410977828\n",
      "Val Epoch Loss: 0.023122291742382867\n",
      "Training Epoch : [65/1000]===============================================================================\n",
      "Training Epoch Loss: 0.019381837796811994\n",
      "Val Epoch Loss: 0.016057440508647186\n",
      "Training Epoch : [66/1000]===============================================================================\n",
      "Training Epoch Loss: 0.019534916911078126\n",
      "Val Epoch Loss: 0.018217294525943305\n",
      "Training Epoch : [67/1000]===============================================================================\n",
      "Training Epoch Loss: 0.019023133196720953\n",
      "Val Epoch Loss: 0.017306771068098515\n",
      "Training Epoch : [68/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01989968233790837\n",
      "Val Epoch Loss: 0.022443993317306433\n",
      "Training Epoch : [69/1000]===============================================================================\n",
      "Training Epoch Loss: 0.033936024275853446\n",
      "Val Epoch Loss: 0.02962270158490068\n",
      "Training Epoch : [70/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02304939219826146\n",
      "Val Epoch Loss: 0.020698931827945143\n",
      "Training Epoch : [71/1000]===============================================================================\n",
      "Training Epoch Loss: 0.018967928070771068\n",
      "Val Epoch Loss: 0.019822269592336135\n",
      "Training Epoch : [72/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01712881361967639\n",
      "Val Epoch Loss: 0.01907670519339215\n",
      "Training Epoch : [73/1000]===============================================================================\n",
      "Training Epoch Loss: 0.021116518979205898\n",
      "Val Epoch Loss: 0.016090550337378915\n",
      "Training Epoch : [74/1000]===============================================================================\n",
      "Training Epoch Loss: 0.020135189970269016\n",
      "Val Epoch Loss: 0.015374872251413763\n",
      "Training Epoch : [75/1000]===============================================================================\n",
      "Training Epoch Loss: 0.019563000477654368\n",
      "Val Epoch Loss: 0.016148803057149053\n",
      "Training Epoch : [76/1000]===============================================================================\n",
      "Training Epoch Loss: 0.019268715533574943\n",
      "Val Epoch Loss: 0.02279902410105263\n",
      "Training Epoch : [77/1000]===============================================================================\n",
      "Training Epoch Loss: 0.0190392547090979\n",
      "Val Epoch Loss: 0.022350194392186638\n",
      "Training Epoch : [78/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01861117917456125\n",
      "Val Epoch Loss: 0.023773483221272106\n",
      "Training Epoch : [79/1000]===============================================================================\n",
      "Training Epoch Loss: 0.026473477066151406\n",
      "Val Epoch Loss: 0.037244177938095833\n",
      "Training Epoch : [80/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02475360202554025\n",
      "Val Epoch Loss: 0.015530228522900296\n",
      "Training Epoch : [81/1000]===============================================================================\n",
      "Training Epoch Loss: 0.018402811973110625\n",
      "Val Epoch Loss: 0.019688574321518996\n",
      "Training Epoch : [82/1000]===============================================================================\n",
      "Training Epoch Loss: 0.018103100874118115\n",
      "Val Epoch Loss: 0.02060187698991381\n",
      "Training Epoch : [83/1000]===============================================================================\n",
      "Training Epoch Loss: 0.018427595226584298\n",
      "Val Epoch Loss: 0.01610274061768953\n",
      "Training Epoch : [84/1000]===============================================================================\n",
      "Training Epoch Loss: 0.018273529302524894\n",
      "Val Epoch Loss: 0.01733638457708845\n",
      "Training Epoch : [85/1000]===============================================================================\n",
      "Training Epoch Loss: 0.019286631974146554\n",
      "Val Epoch Loss: 0.025526609357544465\n",
      "Training Epoch : [86/1000]===============================================================================\n",
      "Training Epoch Loss: 0.018489518521451635\n",
      "Val Epoch Loss: 0.021591258713191275\n",
      "Training Epoch : [87/1000]===============================================================================\n",
      "Training Epoch Loss: 0.019224301372703752\n",
      "Val Epoch Loss: 0.017495407355263046\n",
      "Training Epoch : [88/1000]===============================================================================\n",
      "Training Epoch Loss: 0.019945457798281784\n",
      "Val Epoch Loss: 0.01965642640808303\n",
      "Training Epoch : [89/1000]===============================================================================\n",
      "Training Epoch Loss: 0.021147138194033976\n",
      "Val Epoch Loss: 0.025228945832503468\n",
      "Training Epoch : [90/1000]===============================================================================\n",
      "Training Epoch Loss: 0.020227466905979735\n",
      "Val Epoch Loss: 0.01912842148694357\n",
      "Training Epoch : [91/1000]===============================================================================\n",
      "Training Epoch Loss: 0.021234355474773207\n",
      "Val Epoch Loss: 0.026776036210848314\n",
      "Training Epoch : [92/1000]===============================================================================\n",
      "Training Epoch Loss: 0.021917468034907392\n",
      "Val Epoch Loss: 0.02712528772750183\n",
      "Training Epoch : [93/1000]===============================================================================\n",
      "Training Epoch Loss: 0.020448661188742046\n",
      "Val Epoch Loss: 0.016923028442665543\n",
      "Training Epoch : [94/1000]===============================================================================\n",
      "Training Epoch Loss: 0.022104306342570407\n",
      "Val Epoch Loss: 0.02968188114219198\n",
      "Training Epoch : [95/1000]===============================================================================\n",
      "Training Epoch Loss: 0.018553973322636204\n",
      "Val Epoch Loss: 0.01367541700662849\n",
      "Saving Best Model =======================================>\n",
      "Training Epoch : [96/1000]===============================================================================\n",
      "Training Epoch Loss: 0.018626287432485504\n",
      "Val Epoch Loss: 0.03012149818976851\n",
      "Training Epoch : [97/1000]===============================================================================\n",
      "Training Epoch Loss: 0.019615159664106995\n",
      "Val Epoch Loss: 0.015412845648825169\n",
      "Training Epoch : [98/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01873768518041623\n",
      "Val Epoch Loss: 0.02217416391756974\n",
      "Training Epoch : [99/1000]===============================================================================\n",
      "Training Epoch Loss: 0.026242515660430257\n",
      "Val Epoch Loss: 0.02151888249883134\n",
      "Training Epoch : [100/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01810589158221295\n",
      "Val Epoch Loss: 0.01681117414141466\n",
      "Training Epoch : [101/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01816997593758922\n",
      "Val Epoch Loss: 0.027758418790701973\n",
      "Training Epoch : [102/1000]===============================================================================\n",
      "Training Epoch Loss: 0.028385689874228678\n",
      "Val Epoch Loss: 0.020807763947615104\n",
      "Training Epoch : [103/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02265827156799404\n",
      "Val Epoch Loss: 0.015323569851094171\n",
      "Training Epoch : [104/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01930595439319548\n",
      "Val Epoch Loss: 0.01896962319427219\n",
      "Training Epoch : [105/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01805920876856697\n",
      "Val Epoch Loss: 0.01656130818944228\n",
      "Training Epoch : [106/1000]===============================================================================\n",
      "Training Epoch Loss: 0.017931322640690365\n",
      "Val Epoch Loss: 0.020629502079253525\n",
      "Training Epoch : [107/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01672350085879627\n",
      "Val Epoch Loss: 0.014187924134986181\n",
      "Training Epoch : [108/1000]===============================================================================\n",
      "Training Epoch Loss: 0.021318147326574513\n",
      "Val Epoch Loss: 0.018475942289162623\n",
      "Training Epoch : [109/1000]===============================================================================\n",
      "Training Epoch Loss: 0.026835355025373007\n",
      "Val Epoch Loss: 0.01814338200308971\n",
      "Training Epoch : [110/1000]===============================================================================\n",
      "Training Epoch Loss: 0.021077784145937153\n",
      "Val Epoch Loss: 0.025373610839443773\n",
      "Training Epoch : [111/1000]===============================================================================\n",
      "Training Epoch Loss: 0.021294762699031515\n",
      "Val Epoch Loss: 0.016641178486966772\n",
      "Training Epoch : [112/1000]===============================================================================\n",
      "Training Epoch Loss: 0.019247986915472307\n",
      "Val Epoch Loss: 0.017706579320426834\n",
      "Training Epoch : [113/1000]===============================================================================\n",
      "Training Epoch Loss: 0.020492800854538615\n",
      "Val Epoch Loss: 0.01827622557328524\n",
      "Training Epoch : [114/1000]===============================================================================\n",
      "Training Epoch Loss: 0.017869415791019014\n",
      "Val Epoch Loss: 0.017480985236991393\n",
      "Training Epoch : [115/1000]===============================================================================\n",
      "Training Epoch Loss: 0.0163953931463\n",
      "Val Epoch Loss: 0.01403741695378956\n",
      "Training Epoch : [116/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01635945610407936\n",
      "Val Epoch Loss: 0.018546114289691967\n",
      "Training Epoch : [117/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01584298778815489\n",
      "Val Epoch Loss: 0.01727556239041549\n",
      "Training Epoch : [118/1000]===============================================================================\n",
      "Training Epoch Loss: 0.017302473467823706\n",
      "Val Epoch Loss: 0.022169918379452275\n",
      "Training Epoch : [119/1000]===============================================================================\n",
      "Training Epoch Loss: 0.018010887611461312\n",
      "Val Epoch Loss: 0.01617721709006123\n",
      "Training Epoch : [120/1000]===============================================================================\n",
      "Training Epoch Loss: 0.017218575004096095\n",
      "Val Epoch Loss: 0.021226414960294374\n",
      "Training Epoch : [121/1000]===============================================================================\n",
      "Training Epoch Loss: 0.018034718808178838\n",
      "Val Epoch Loss: 0.0200144102135183\n",
      "Training Epoch : [122/1000]===============================================================================\n",
      "Training Epoch Loss: 0.017424086353888635\n",
      "Val Epoch Loss: 0.024521029370493795\n",
      "Training Epoch : [123/1000]===============================================================================\n",
      "Training Epoch Loss: 0.020741021236110675\n",
      "Val Epoch Loss: 0.026903058120392655\n",
      "Training Epoch : [124/1000]===============================================================================\n",
      "Training Epoch Loss: 0.020051725723437573\n",
      "Val Epoch Loss: 0.016703623824899917\n",
      "Training Epoch : [125/1000]===============================================================================\n",
      "Training Epoch Loss: 0.019601162915167055\n",
      "Val Epoch Loss: 0.02126082266974998\n",
      "Training Epoch : [126/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016799312648608497\n",
      "Val Epoch Loss: 0.01853806919442784\n",
      "Training Epoch : [127/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016839728592649886\n",
      "Val Epoch Loss: 0.014960639001066355\n",
      "Training Epoch : [128/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01842168953857924\n",
      "Val Epoch Loss: 0.019608768315887766\n",
      "Training Epoch : [129/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016601054242959146\n",
      "Val Epoch Loss: 0.015238507255593217\n",
      "Training Epoch : [130/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01695829715677782\n",
      "Val Epoch Loss: 0.020624430366105548\n",
      "Training Epoch : [131/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01586894610112435\n",
      "Val Epoch Loss: 0.022269692180041028\n",
      "Training Epoch : [132/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016750778463718138\n",
      "Val Epoch Loss: 0.022887509781867266\n",
      "Training Epoch : [133/1000]===============================================================================\n",
      "Training Epoch Loss: 0.021385507413039084\n",
      "Val Epoch Loss: 0.020318134834891872\n",
      "Training Epoch : [134/1000]===============================================================================\n",
      "Training Epoch Loss: 0.03095217536840784\n",
      "Val Epoch Loss: 0.016964146695835024\n",
      "Training Epoch : [135/1000]===============================================================================\n",
      "Training Epoch Loss: 0.018497095467816842\n",
      "Val Epoch Loss: 0.016417104493532526\n",
      "Training Epoch : [136/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016654754866306718\n",
      "Val Epoch Loss: 0.019786400524409192\n",
      "Training Epoch : [137/1000]===============================================================================\n",
      "Training Epoch Loss: 0.017865799612512712\n",
      "Val Epoch Loss: 0.019271319272535805\n",
      "Training Epoch : [138/1000]===============================================================================\n",
      "Training Epoch Loss: 0.018777305064232724\n",
      "Val Epoch Loss: 0.02089950729063467\n",
      "Training Epoch : [139/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01651363390939016\n",
      "Val Epoch Loss: 0.021951984748930523\n",
      "Training Epoch : [140/1000]===============================================================================\n",
      "Training Epoch Loss: 0.019759085960686207\n",
      "Val Epoch Loss: 0.02218569957308079\n",
      "Training Epoch : [141/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015885919945216494\n",
      "Val Epoch Loss: 0.015004866716672519\n",
      "Training Epoch : [142/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01698236057142678\n",
      "Val Epoch Loss: 0.018651259284907656\n",
      "Training Epoch : [143/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01579079911799023\n",
      "Val Epoch Loss: 0.035791611911630945\n",
      "Training Epoch : [144/1000]===============================================================================\n",
      "Training Epoch Loss: 0.021399495780075852\n",
      "Val Epoch Loss: 0.019520882498718015\n",
      "Training Epoch : [145/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016683469207859354\n",
      "Val Epoch Loss: 0.020912135442669847\n",
      "Training Epoch : [146/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016371440951173242\n",
      "Val Epoch Loss: 0.017226122099121933\n",
      "Training Epoch : [147/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016111434314792093\n",
      "Val Epoch Loss: 0.015154837217108396\n",
      "Training Epoch : [148/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01789391116778317\n",
      "Val Epoch Loss: 0.014621432735829762\n",
      "Training Epoch : [149/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016734124308353977\n",
      "Val Epoch Loss: 0.01942620479109648\n",
      "Training Epoch : [150/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016661589401529023\n",
      "Val Epoch Loss: 0.014875208468813645\n",
      "Training Epoch : [151/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016989217458390875\n",
      "Val Epoch Loss: 0.014292304964711596\n",
      "Training Epoch : [152/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015415065309130832\n",
      "Val Epoch Loss: 0.015496349433976176\n",
      "Training Epoch : [153/1000]===============================================================================\n",
      "Training Epoch Loss: 0.0155730489641428\n",
      "Val Epoch Loss: 0.019891733340428847\n",
      "Training Epoch : [154/1000]===============================================================================\n",
      "Training Epoch Loss: 0.017418065672054103\n",
      "Val Epoch Loss: 0.018149998222820853\n",
      "Training Epoch : [155/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01639410939165636\n",
      "Val Epoch Loss: 0.02133208840448213\n",
      "Training Epoch : [156/1000]===============================================================================\n",
      "Training Epoch Loss: 0.019432514489285256\n",
      "Val Epoch Loss: 0.018138683986188352\n",
      "Training Epoch : [157/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014805160193262916\n",
      "Val Epoch Loss: 0.02024678960687628\n",
      "Training Epoch : [158/1000]===============================================================================\n",
      "Training Epoch Loss: 0.023705547503930956\n",
      "Val Epoch Loss: 0.02121175987351882\n",
      "Training Epoch : [159/1000]===============================================================================\n",
      "Training Epoch Loss: 0.018562631362951117\n",
      "Val Epoch Loss: 0.021083306827533402\n",
      "Training Epoch : [160/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01865461985825708\n",
      "Val Epoch Loss: 0.0254480086242486\n",
      "Training Epoch : [161/1000]===============================================================================\n",
      "Training Epoch Loss: 0.020798264635040573\n",
      "Val Epoch Loss: 0.02444273496879951\n",
      "Training Epoch : [162/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016247488656326345\n",
      "Val Epoch Loss: 0.01710245215432032\n",
      "Training Epoch : [163/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016280506222851966\n",
      "Val Epoch Loss: 0.01529349191543205\n",
      "Training Epoch : [164/1000]===============================================================================\n",
      "Training Epoch Loss: 0.022396518015547803\n",
      "Val Epoch Loss: 0.023233571874075813\n",
      "Training Epoch : [165/1000]===============================================================================\n",
      "Training Epoch Loss: 0.017623657899859705\n",
      "Val Epoch Loss: 0.03809495179570819\n",
      "Training Epoch : [166/1000]===============================================================================\n",
      "Training Epoch Loss: 0.022400759405603533\n",
      "Val Epoch Loss: 0.01963225252142078\n",
      "Training Epoch : [167/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016267778302885984\n",
      "Val Epoch Loss: 0.017031702978879605\n",
      "Training Epoch : [168/1000]===============================================================================\n",
      "Training Epoch Loss: 0.0192396704009489\n",
      "Val Epoch Loss: 0.02910264735845359\n",
      "Training Epoch : [169/1000]===============================================================================\n",
      "Training Epoch Loss: 0.023321910834822216\n",
      "Val Epoch Loss: 0.02234331631150685\n",
      "Training Epoch : [170/1000]===============================================================================\n",
      "Training Epoch Loss: 0.017509665707812497\n",
      "Val Epoch Loss: 0.024179453839008744\n",
      "Training Epoch : [171/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01582271519950346\n",
      "Val Epoch Loss: 0.015895649202560123\n",
      "Training Epoch : [172/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016894312477425524\n",
      "Val Epoch Loss: 0.020888184068577464\n",
      "Training Epoch : [173/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02810144583743654\n",
      "Val Epoch Loss: 0.025048487292798727\n",
      "Training Epoch : [174/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01868449761777332\n",
      "Val Epoch Loss: 0.019650741014629602\n",
      "Training Epoch : [175/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01846181142977194\n",
      "Val Epoch Loss: 0.02351668182956545\n",
      "Training Epoch : [176/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015488573006893458\n",
      "Val Epoch Loss: 0.014352222279596486\n",
      "Training Epoch : [177/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015519996884426004\n",
      "Val Epoch Loss: 0.01654236972344255\n",
      "Training Epoch : [178/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01495097388856505\n",
      "Val Epoch Loss: 0.022197891994820612\n",
      "Training Epoch : [179/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01522524324000666\n",
      "Val Epoch Loss: 0.018567004314574757\n",
      "Training Epoch : [180/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01525695798428435\n",
      "Val Epoch Loss: 0.02028004714214292\n",
      "Training Epoch : [181/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015421700418779724\n",
      "Val Epoch Loss: 0.013532987979583834\n",
      "Saving Best Model =======================================>\n",
      "Training Epoch : [182/1000]===============================================================================\n",
      "Training Epoch Loss: 0.017697869432403854\n",
      "Val Epoch Loss: 0.017876941600422327\n",
      "Training Epoch : [183/1000]===============================================================================\n",
      "Training Epoch Loss: 0.022172140349683008\n",
      "Val Epoch Loss: 0.04026970062649956\n",
      "Training Epoch : [184/1000]===============================================================================\n",
      "Training Epoch Loss: 0.021056152414530516\n",
      "Val Epoch Loss: 0.01787740785910405\n",
      "Training Epoch : [185/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01598847755476048\n",
      "Val Epoch Loss: 0.019406002333485765\n",
      "Training Epoch : [186/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015749624856796703\n",
      "Val Epoch Loss: 0.023655547662393042\n",
      "Training Epoch : [187/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014663586442015673\n",
      "Val Epoch Loss: 0.015998673965345676\n",
      "Training Epoch : [188/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015128265306549636\n",
      "Val Epoch Loss: 0.01727031010178555\n",
      "Training Epoch : [189/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015403815961786006\n",
      "Val Epoch Loss: 0.014733802254532316\n",
      "Training Epoch : [190/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01475181695269911\n",
      "Val Epoch Loss: 0.014913829521423108\n",
      "Training Epoch : [191/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01704275203672679\n",
      "Val Epoch Loss: 0.01558602684618611\n",
      "Training Epoch : [192/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015276172958118351\n",
      "Val Epoch Loss: 0.020216523476655743\n",
      "Training Epoch : [193/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015399076387678323\n",
      "Val Epoch Loss: 0.015814427004538868\n",
      "Training Epoch : [194/1000]===============================================================================\n",
      "Training Epoch Loss: 0.017306891466049773\n",
      "Val Epoch Loss: 0.027929030199486175\n",
      "Training Epoch : [195/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015609807138772388\n",
      "Val Epoch Loss: 0.0157363583168358\n",
      "Training Epoch : [196/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015302885960983602\n",
      "Val Epoch Loss: 0.02287451845079072\n",
      "Training Epoch : [197/1000]===============================================================================\n",
      "Training Epoch Loss: 0.017608550254647668\n",
      "Val Epoch Loss: 0.017806243243333148\n",
      "Training Epoch : [198/1000]===============================================================================\n",
      "Training Epoch Loss: 0.018208466127122704\n",
      "Val Epoch Loss: 0.023400282854900548\n",
      "Training Epoch : [199/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015507151495273175\n",
      "Val Epoch Loss: 0.021930337387235148\n",
      "Training Epoch : [200/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01849352352713284\n",
      "Val Epoch Loss: 0.0164092955957657\n",
      "Training Epoch : [201/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014607057612585394\n",
      "Val Epoch Loss: 0.021220064954832196\n",
      "Training Epoch : [202/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01632324584122551\n",
      "Val Epoch Loss: 0.018366688684756428\n",
      "Training Epoch : [203/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016475165654954157\n",
      "Val Epoch Loss: 0.02011350249430459\n",
      "Training Epoch : [204/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015083608585164734\n",
      "Val Epoch Loss: 0.019022224863108835\n",
      "Training Epoch : [205/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014284758044308737\n",
      "Val Epoch Loss: 0.01902376451701122\n",
      "Training Epoch : [206/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015940741390774126\n",
      "Val Epoch Loss: 0.01738454322453196\n",
      "Training Epoch : [207/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014564409190298696\n",
      "Val Epoch Loss: 0.015451968586268393\n",
      "Training Epoch : [208/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013544302948407437\n",
      "Val Epoch Loss: 0.02570391419086311\n",
      "Training Epoch : [209/1000]===============================================================================\n",
      "Training Epoch Loss: 0.0145429676202567\n",
      "Val Epoch Loss: 0.01575977473188878\n",
      "Training Epoch : [210/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01505979728934012\n",
      "Val Epoch Loss: 0.023560646284175545\n",
      "Training Epoch : [211/1000]===============================================================================\n",
      "Training Epoch Loss: 0.017967487758907833\n",
      "Val Epoch Loss: 0.02553048832219486\n",
      "Training Epoch : [212/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02171803550108483\n",
      "Val Epoch Loss: 0.023650094229531914\n",
      "Training Epoch : [213/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016070058789888497\n",
      "Val Epoch Loss: 0.016035952442938362\n",
      "Training Epoch : [214/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014133217989614135\n",
      "Val Epoch Loss: 0.015012800763973868\n",
      "Training Epoch : [215/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014910350455657431\n",
      "Val Epoch Loss: 0.015466826232640367\n",
      "Training Epoch : [216/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015481650045043543\n",
      "Val Epoch Loss: 0.015959946959475547\n",
      "Training Epoch : [217/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015227219065357196\n",
      "Val Epoch Loss: 0.01789717751212026\n",
      "Training Epoch : [218/1000]===============================================================================\n",
      "Training Epoch Loss: 0.020133693628993473\n",
      "Val Epoch Loss: 0.026775685338131888\n",
      "Training Epoch : [219/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01682132492332082\n",
      "Val Epoch Loss: 0.016604595437743945\n",
      "Training Epoch : [220/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014464669542289093\n",
      "Val Epoch Loss: 0.017541064160851466\n",
      "Training Epoch : [221/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016055915529202474\n",
      "Val Epoch Loss: 0.02343078740676375\n",
      "Training Epoch : [222/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01679212092667034\n",
      "Val Epoch Loss: 0.018168438973493482\n",
      "Training Epoch : [223/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016182912342054278\n",
      "Val Epoch Loss: 0.015613483568334854\n",
      "Training Epoch : [224/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014936544442255246\n",
      "Val Epoch Loss: 0.018421029947747133\n",
      "Training Epoch : [225/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014686102831834242\n",
      "Val Epoch Loss: 0.015314644925590409\n",
      "Training Epoch : [226/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013300218570389245\n",
      "Val Epoch Loss: 0.02083805821664435\n",
      "Training Epoch : [227/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013887574406046616\n",
      "Val Epoch Loss: 0.016820979041145427\n",
      "Training Epoch : [228/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014652611442694538\n",
      "Val Epoch Loss: 0.01590632425474101\n",
      "Training Epoch : [229/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014252402844201578\n",
      "Val Epoch Loss: 0.01242666038084089\n",
      "Saving Best Model =======================================>\n",
      "Training Epoch : [230/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014777214486936205\n",
      "Val Epoch Loss: 0.018791837761742307\n",
      "Training Epoch : [231/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014312098170385548\n",
      "Val Epoch Loss: 0.013964263406753736\n",
      "Training Epoch : [232/1000]===============================================================================\n",
      "Training Epoch Loss: 0.0158337118643287\n",
      "Val Epoch Loss: 0.020736841834150255\n",
      "Training Epoch : [233/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01439653082113517\n",
      "Val Epoch Loss: 0.01565482440453611\n",
      "Training Epoch : [234/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01416397986835555\n",
      "Val Epoch Loss: 0.021463211772865372\n",
      "Training Epoch : [235/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015790745003246946\n",
      "Val Epoch Loss: 0.01697758602983269\n",
      "Training Epoch : [236/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014327941339855132\n",
      "Val Epoch Loss: 0.018139116625350556\n",
      "Training Epoch : [237/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014415060660164607\n",
      "Val Epoch Loss: 0.014459435262803086\n",
      "Training Epoch : [238/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014155643305888302\n",
      "Val Epoch Loss: 0.017607643673392504\n",
      "Training Epoch : [239/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015077113860139721\n",
      "Val Epoch Loss: 0.015026558864567625\n",
      "Training Epoch : [240/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014510392677038908\n",
      "Val Epoch Loss: 0.020705987907652008\n",
      "Training Epoch : [241/1000]===============================================================================\n",
      "Training Epoch Loss: 0.018074014591739365\n",
      "Val Epoch Loss: 0.02327154582905534\n",
      "Training Epoch : [242/1000]===============================================================================\n",
      "Training Epoch Loss: 0.018999241439527588\n",
      "Val Epoch Loss: 0.014651234215738154\n",
      "Training Epoch : [243/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014587506504827425\n",
      "Val Epoch Loss: 0.017760044919621003\n",
      "Training Epoch : [244/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015692186909482667\n",
      "Val Epoch Loss: 0.017414244139983662\n",
      "Training Epoch : [245/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01672999754450039\n",
      "Val Epoch Loss: 0.023110793670639396\n",
      "Training Epoch : [246/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016518106929173593\n",
      "Val Epoch Loss: 0.015020185036854329\n",
      "Training Epoch : [247/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015555445153854396\n",
      "Val Epoch Loss: 0.021709502533715414\n",
      "Training Epoch : [248/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013646637457177827\n",
      "Val Epoch Loss: 0.01870294949989521\n",
      "Training Epoch : [249/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014070897804279076\n",
      "Val Epoch Loss: 0.015025753708629819\n",
      "Training Epoch : [250/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013974384502752832\n",
      "Val Epoch Loss: 0.018179323704986785\n",
      "Training Epoch : [251/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014853648424736764\n",
      "Val Epoch Loss: 0.021956681702776176\n",
      "Training Epoch : [252/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015613170977877943\n",
      "Val Epoch Loss: 0.018293095911902032\n",
      "Training Epoch : [253/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014930290341573326\n",
      "Val Epoch Loss: 0.017828766225935204\n",
      "Training Epoch : [254/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014685227119020726\n",
      "Val Epoch Loss: 0.018283465056140955\n",
      "Training Epoch : [255/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013809281744455037\n",
      "Val Epoch Loss: 0.016799042506854198\n",
      "Training Epoch : [256/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01274275492974802\n",
      "Val Epoch Loss: 0.017364941500617487\n",
      "Training Epoch : [257/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01949369037327798\n",
      "Val Epoch Loss: 0.019658627950488346\n",
      "Training Epoch : [258/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013567539765254447\n",
      "Val Epoch Loss: 0.022250488657798422\n",
      "Training Epoch : [259/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015353455575869271\n",
      "Val Epoch Loss: 0.018539258217635125\n",
      "Training Epoch : [260/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014777366071939468\n",
      "Val Epoch Loss: 0.014800768670331883\n",
      "Training Epoch : [261/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016520308992384297\n",
      "Val Epoch Loss: 0.01690775736872303\n",
      "Training Epoch : [262/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01660140076848237\n",
      "Val Epoch Loss: 0.018416916908647277\n",
      "Training Epoch : [263/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015971311637641566\n",
      "Val Epoch Loss: 0.018373624654486775\n",
      "Training Epoch : [264/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014400606950450884\n",
      "Val Epoch Loss: 0.017753769181007028\n",
      "Training Epoch : [265/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014013419908128287\n",
      "Val Epoch Loss: 0.011880229754177364\n",
      "Saving Best Model =======================================>\n",
      "Training Epoch : [266/1000]===============================================================================\n",
      "Training Epoch Loss: 0.018497185953157514\n",
      "Val Epoch Loss: 0.013436575895052795\n",
      "Training Epoch : [267/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01915568552028976\n",
      "Val Epoch Loss: 0.02675807270172395\n",
      "Training Epoch : [268/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015627190495203985\n",
      "Val Epoch Loss: 0.017228031059187885\n",
      "Training Epoch : [269/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01492095233774499\n",
      "Val Epoch Loss: 0.01970379377445696\n",
      "Training Epoch : [270/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01473728314924397\n",
      "Val Epoch Loss: 0.01733639420892455\n",
      "Training Epoch : [271/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014778781094049153\n",
      "Val Epoch Loss: 0.018659292720258236\n",
      "Training Epoch : [272/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015685931436325375\n",
      "Val Epoch Loss: 0.023645132510481698\n",
      "Training Epoch : [273/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01612784412052286\n",
      "Val Epoch Loss: 0.024879280520652077\n",
      "Training Epoch : [274/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01680429809187588\n",
      "Val Epoch Loss: 0.023827546018813\n",
      "Training Epoch : [275/1000]===============================================================================\n",
      "Training Epoch Loss: 0.0151626717201189\n",
      "Val Epoch Loss: 0.027305702286723414\n",
      "Training Epoch : [276/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015329836622664803\n",
      "Val Epoch Loss: 0.020332884907379355\n",
      "Training Epoch : [277/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014623309755207677\n",
      "Val Epoch Loss: 0.024979282918043043\n",
      "Training Epoch : [278/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013609206254937146\n",
      "Val Epoch Loss: 0.018765265220089963\n",
      "Training Epoch : [279/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013870782611009321\n",
      "Val Epoch Loss: 0.01798819786850608\n",
      "Training Epoch : [280/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014356393416069056\n",
      "Val Epoch Loss: 0.01890257168499949\n",
      "Training Epoch : [281/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01284492773150927\n",
      "Val Epoch Loss: 0.02011483106272001\n",
      "Training Epoch : [282/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013601523529934255\n",
      "Val Epoch Loss: 0.01472240937015924\n",
      "Training Epoch : [283/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016925519773442494\n",
      "Val Epoch Loss: 0.028896991194127815\n",
      "Training Epoch : [284/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014717550776702793\n",
      "Val Epoch Loss: 0.01652421558882404\n",
      "Training Epoch : [285/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01545147872284839\n",
      "Val Epoch Loss: 0.016933776349392964\n",
      "Training Epoch : [286/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014433796934195255\n",
      "Val Epoch Loss: 0.015715664583503416\n",
      "Training Epoch : [287/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013991848688180509\n",
      "Val Epoch Loss: 0.0271407329587658\n",
      "Training Epoch : [288/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013482924746839623\n",
      "Val Epoch Loss: 0.01400320787644504\n",
      "Training Epoch : [289/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016346365213394165\n",
      "Val Epoch Loss: 0.018830535835341403\n",
      "Training Epoch : [290/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015312009662585823\n",
      "Val Epoch Loss: 0.02270071460914455\n",
      "Training Epoch : [291/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015975457665167357\n",
      "Val Epoch Loss: 0.017251667799428105\n",
      "Training Epoch : [292/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015698455225088094\n",
      "Val Epoch Loss: 0.016680389048384602\n",
      "Training Epoch : [293/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014856827616887657\n",
      "Val Epoch Loss: 0.01815514761102876\n",
      "Training Epoch : [294/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014237577683831515\n",
      "Val Epoch Loss: 0.021513242789201047\n",
      "Training Epoch : [295/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014579184034741238\n",
      "Val Epoch Loss: 0.027269353391602635\n",
      "Training Epoch : [296/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014370078809167209\n",
      "Val Epoch Loss: 0.015708292338163836\n",
      "Training Epoch : [297/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01274677248377549\n",
      "Val Epoch Loss: 0.013962477128523844\n",
      "Training Epoch : [298/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01306682067776197\n",
      "Val Epoch Loss: 0.016436027623345388\n",
      "Training Epoch : [299/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012758616529601184\n",
      "Val Epoch Loss: 0.014435265047818814\n",
      "Training Epoch : [300/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013053876004720988\n",
      "Val Epoch Loss: 0.0196401966754102\n",
      "Training Epoch : [301/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013611676342981426\n",
      "Val Epoch Loss: 0.013951700686256549\n",
      "Training Epoch : [302/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016650570360453504\n",
      "Val Epoch Loss: 0.01736181665604052\n",
      "Training Epoch : [303/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013284375400919663\n",
      "Val Epoch Loss: 0.021079060155898333\n",
      "Training Epoch : [304/1000]===============================================================================\n",
      "Training Epoch Loss: 0.024562321725840632\n",
      "Val Epoch Loss: 0.022742637874264466\n",
      "Training Epoch : [305/1000]===============================================================================\n",
      "Training Epoch Loss: 0.02186084256850575\n",
      "Val Epoch Loss: 0.02728685221977924\n",
      "Training Epoch : [306/1000]===============================================================================\n",
      "Training Epoch Loss: 0.020464009311246246\n",
      "Val Epoch Loss: 0.02048580105244917\n",
      "Training Epoch : [307/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016998573230873597\n",
      "Val Epoch Loss: 0.020333239557466617\n",
      "Training Epoch : [308/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016829734177965867\n",
      "Val Epoch Loss: 0.017466160977308295\n",
      "Training Epoch : [309/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014694136444871364\n",
      "Val Epoch Loss: 0.019776673782249225\n",
      "Training Epoch : [310/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01521575075917338\n",
      "Val Epoch Loss: 0.014790345145708048\n",
      "Training Epoch : [311/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014103734179546959\n",
      "Val Epoch Loss: 0.01573876021000998\n",
      "Training Epoch : [312/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013843386237950702\n",
      "Val Epoch Loss: 0.020846511316975874\n",
      "Training Epoch : [313/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013198730045635449\n",
      "Val Epoch Loss: 0.020142856611576127\n",
      "Training Epoch : [314/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01664030461228992\n",
      "Val Epoch Loss: 0.017339186583596625\n",
      "Training Epoch : [315/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01384567185059974\n",
      "Val Epoch Loss: 0.01761596350268902\n",
      "Training Epoch : [316/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013691648987955168\n",
      "Val Epoch Loss: 0.019760960731410274\n",
      "Training Epoch : [317/1000]===============================================================================\n",
      "Training Epoch Loss: 0.017366511992325907\n",
      "Val Epoch Loss: 0.021798886965323044\n",
      "Training Epoch : [318/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015138688595279268\n",
      "Val Epoch Loss: 0.019798525609076023\n",
      "Training Epoch : [319/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014242912618149268\n",
      "Val Epoch Loss: 0.018772868442618728\n",
      "Training Epoch : [320/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014263761058253678\n",
      "Val Epoch Loss: 0.01572573365717146\n",
      "Training Epoch : [321/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013458375181806715\n",
      "Val Epoch Loss: 0.01627612894241649\n",
      "Training Epoch : [322/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012901031142590861\n",
      "Val Epoch Loss: 0.017108115718369128\n",
      "Training Epoch : [323/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013681463652143353\n",
      "Val Epoch Loss: 0.01685079348298084\n",
      "Training Epoch : [324/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013153059383560168\n",
      "Val Epoch Loss: 0.01583545371326373\n",
      "Training Epoch : [325/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013102460235945489\n",
      "Val Epoch Loss: 0.022801065952272007\n",
      "Training Epoch : [326/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012701901251842318\n",
      "Val Epoch Loss: 0.017677743274314133\n",
      "Training Epoch : [327/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01395063776228773\n",
      "Val Epoch Loss: 0.028118825149967483\n",
      "Training Epoch : [328/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01624834770336747\n",
      "Val Epoch Loss: 0.020719486460285753\n",
      "Training Epoch : [329/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014927573967725039\n",
      "Val Epoch Loss: 0.01788284426469258\n",
      "Training Epoch : [330/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013360546607720224\n",
      "Val Epoch Loss: 0.01980603275220155\n",
      "Training Epoch : [331/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01329099504571212\n",
      "Val Epoch Loss: 0.01665174084605257\n",
      "Training Epoch : [332/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013546576919524293\n",
      "Val Epoch Loss: 0.013425464052584414\n",
      "Training Epoch : [333/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013667931363574769\n",
      "Val Epoch Loss: 0.013154810187284295\n",
      "Training Epoch : [334/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014961268311660541\n",
      "Val Epoch Loss: 0.014325950000631181\n",
      "Training Epoch : [335/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014256102792722615\n",
      "Val Epoch Loss: 0.014500975060448246\n",
      "Training Epoch : [336/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014078907889166945\n",
      "Val Epoch Loss: 0.014747242296212599\n",
      "Training Epoch : [337/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012800828602753188\n",
      "Val Epoch Loss: 0.016111947828903794\n",
      "Training Epoch : [338/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01421227497293761\n",
      "Val Epoch Loss: 0.016260099401207345\n",
      "Training Epoch : [339/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013964150297014337\n",
      "Val Epoch Loss: 0.0162875908855839\n",
      "Training Epoch : [340/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013408171429641937\n",
      "Val Epoch Loss: 0.013918954266333267\n",
      "Training Epoch : [341/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015680813549184484\n",
      "Val Epoch Loss: 0.014986647075458773\n",
      "Training Epoch : [342/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013789207772596887\n",
      "Val Epoch Loss: 0.02259141902782415\n",
      "Training Epoch : [343/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014240426207451444\n",
      "Val Epoch Loss: 0.021926649276314204\n",
      "Training Epoch : [344/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014446471058028309\n",
      "Val Epoch Loss: 0.01772129425386849\n",
      "Training Epoch : [345/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012744560421101357\n",
      "Val Epoch Loss: 0.015862234889554155\n",
      "Training Epoch : [346/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012799900270214206\n",
      "Val Epoch Loss: 0.01879428787843177\n",
      "Training Epoch : [347/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014185891854331681\n",
      "Val Epoch Loss: 0.018409467372724687\n",
      "Training Epoch : [348/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015300249141689978\n",
      "Val Epoch Loss: 0.019320337340774898\n",
      "Training Epoch : [349/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016437599092329804\n",
      "Val Epoch Loss: 0.02209317241795361\n",
      "Training Epoch : [350/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01498791837672654\n",
      "Val Epoch Loss: 0.013055369737374253\n",
      "Training Epoch : [351/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013806128604827743\n",
      "Val Epoch Loss: 0.016585164648284644\n",
      "Training Epoch : [352/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014307970524226365\n",
      "Val Epoch Loss: 0.015548518362553105\n",
      "Training Epoch : [353/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012563942833558508\n",
      "Val Epoch Loss: 0.015962869946011586\n",
      "Training Epoch : [354/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013939927724239073\n",
      "Val Epoch Loss: 0.015559569325935291\n",
      "Training Epoch : [355/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013919390934078317\n",
      "Val Epoch Loss: 0.016475238537072744\n",
      "Training Epoch : [356/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013614624763201726\n",
      "Val Epoch Loss: 0.01865915183933746\n",
      "Training Epoch : [357/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013064667484477946\n",
      "Val Epoch Loss: 0.015958070203563886\n",
      "Training Epoch : [358/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012305067435495164\n",
      "Val Epoch Loss: 0.016242956454669565\n",
      "Training Epoch : [359/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013507695108848182\n",
      "Val Epoch Loss: 0.015835454310658144\n",
      "Training Epoch : [360/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014000083368859793\n",
      "Val Epoch Loss: 0.014870848626167955\n",
      "Training Epoch : [361/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013253388313674614\n",
      "Val Epoch Loss: 0.02618408926431776\n",
      "Training Epoch : [362/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015818963464545577\n",
      "Val Epoch Loss: 0.02256816974228346\n",
      "Training Epoch : [363/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013311530389872036\n",
      "Val Epoch Loss: 0.01812236729739724\n",
      "Training Epoch : [364/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012999297487304398\n",
      "Val Epoch Loss: 0.016569523716737564\n",
      "Training Epoch : [365/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015642564768265737\n",
      "Val Epoch Loss: 0.018789497391002153\n",
      "Training Epoch : [366/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015533969369962028\n",
      "Val Epoch Loss: 0.016181803418715533\n",
      "Training Epoch : [367/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013863237256086186\n",
      "Val Epoch Loss: 0.01814038875071626\n",
      "Training Epoch : [368/1000]===============================================================================\n",
      "Training Epoch Loss: 0.020732498948315258\n",
      "Val Epoch Loss: 0.0164430023440601\n",
      "Training Epoch : [369/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014508660466067101\n",
      "Val Epoch Loss: 0.01879718140882783\n",
      "Training Epoch : [370/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014489298539334223\n",
      "Val Epoch Loss: 0.016791254857947167\n",
      "Training Epoch : [371/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01464973792041603\n",
      "Val Epoch Loss: 0.016150743663801176\n",
      "Training Epoch : [372/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013142513625911977\n",
      "Val Epoch Loss: 0.018615631257968123\n",
      "Training Epoch : [373/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013618515289731715\n",
      "Val Epoch Loss: 0.019288310260315866\n",
      "Training Epoch : [374/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01591958073703082\n",
      "Val Epoch Loss: 0.017886121654699213\n",
      "Training Epoch : [375/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014322587141865179\n",
      "Val Epoch Loss: 0.015352382637119215\n",
      "Training Epoch : [376/1000]===============================================================================\n",
      "Training Epoch Loss: 0.017074923104557552\n",
      "Val Epoch Loss: 0.016753205717051106\n",
      "Training Epoch : [377/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01433771990827824\n",
      "Val Epoch Loss: 0.018244414945672218\n",
      "Training Epoch : [378/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015215721520546236\n",
      "Val Epoch Loss: 0.01732247717997157\n",
      "Training Epoch : [379/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014677976380641522\n",
      "Val Epoch Loss: 0.01786459661487147\n",
      "Training Epoch : [380/1000]===============================================================================\n",
      "Training Epoch Loss: 0.025695586736363015\n",
      "Val Epoch Loss: 0.02312156757780988\n",
      "Training Epoch : [381/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016757350169906492\n",
      "Val Epoch Loss: 0.020228471193389084\n",
      "Training Epoch : [382/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015133774790324662\n",
      "Val Epoch Loss: 0.017661883639735414\n",
      "Training Epoch : [383/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014052384483971093\n",
      "Val Epoch Loss: 0.01335976859187021\n",
      "Training Epoch : [384/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013471809044284256\n",
      "Val Epoch Loss: 0.01488860242993963\n",
      "Training Epoch : [385/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013356547091940516\n",
      "Val Epoch Loss: 0.014804906521213093\n",
      "Training Epoch : [386/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013745996954017565\n",
      "Val Epoch Loss: 0.01623705678349851\n",
      "Training Epoch : [387/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01291479197281756\n",
      "Val Epoch Loss: 0.015578640107138964\n",
      "Training Epoch : [388/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01306203152298143\n",
      "Val Epoch Loss: 0.01856724020265239\n",
      "Training Epoch : [389/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013229338231643564\n",
      "Val Epoch Loss: 0.024252476673083084\n",
      "Training Epoch : [390/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015415606848699482\n",
      "Val Epoch Loss: 0.024681620973799574\n",
      "Training Epoch : [391/1000]===============================================================================\n",
      "Training Epoch Loss: 0.022936426715827303\n",
      "Val Epoch Loss: 0.02447021039398877\n",
      "Training Epoch : [392/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014560177554621509\n",
      "Val Epoch Loss: 0.02043158553945097\n",
      "Training Epoch : [393/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014744914896590145\n",
      "Val Epoch Loss: 0.01680590706650707\n",
      "Training Epoch : [394/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012555493090889956\n",
      "Val Epoch Loss: 0.015217918364980602\n",
      "Training Epoch : [395/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01284117614360232\n",
      "Val Epoch Loss: 0.013267180373843172\n",
      "Training Epoch : [396/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015395059936532849\n",
      "Val Epoch Loss: 0.020450244406483284\n",
      "Training Epoch : [397/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014284914702569184\n",
      "Val Epoch Loss: 0.01490111897950747\n",
      "Training Epoch : [398/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013314429033351572\n",
      "Val Epoch Loss: 0.01591600589209089\n",
      "Training Epoch : [399/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012527866460579006\n",
      "Val Epoch Loss: 0.013767598809576348\n",
      "Training Epoch : [400/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013679640147050745\n",
      "Val Epoch Loss: 0.02478443998475804\n",
      "Training Epoch : [401/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014091335678179013\n",
      "Val Epoch Loss: 0.021731968999742286\n",
      "Training Epoch : [402/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013905726422212626\n",
      "Val Epoch Loss: 0.015234873668400963\n",
      "Training Epoch : [403/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013479989046525014\n",
      "Val Epoch Loss: 0.01712206078962864\n",
      "Training Epoch : [404/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012826847598740929\n",
      "Val Epoch Loss: 0.017962370323642205\n",
      "Training Epoch : [405/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013327394244506172\n",
      "Val Epoch Loss: 0.01394004260778035\n",
      "Training Epoch : [406/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01248250002237527\n",
      "Val Epoch Loss: 0.014950053294581411\n",
      "Training Epoch : [407/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012301572071584431\n",
      "Val Epoch Loss: 0.018391399546281287\n",
      "Training Epoch : [408/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014587847981601954\n",
      "Val Epoch Loss: 0.014516809722408652\n",
      "Training Epoch : [409/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01596408749097272\n",
      "Val Epoch Loss: 0.021963303357264714\n",
      "Training Epoch : [410/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015434456229405967\n",
      "Val Epoch Loss: 0.015550020739044013\n",
      "Training Epoch : [411/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013964306293545585\n",
      "Val Epoch Loss: 0.016082345587691588\n",
      "Training Epoch : [412/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012767926299650418\n",
      "Val Epoch Loss: 0.01812732317117288\n",
      "Training Epoch : [413/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012551494114297\n",
      "Val Epoch Loss: 0.015760615136394717\n",
      "Training Epoch : [414/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013269083623431231\n",
      "Val Epoch Loss: 0.017470112796505226\n",
      "Training Epoch : [415/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01666531476535295\n",
      "Val Epoch Loss: 0.0174689811660516\n",
      "Training Epoch : [416/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014025547443643996\n",
      "Val Epoch Loss: 0.015272201167566604\n",
      "Training Epoch : [417/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012949424967365829\n",
      "Val Epoch Loss: 0.01364626595444095\n",
      "Training Epoch : [418/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013229431535460447\n",
      "Val Epoch Loss: 0.014874497182685636\n",
      "Training Epoch : [419/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014634329850148214\n",
      "Val Epoch Loss: 0.019371050460521427\n",
      "Training Epoch : [420/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013421785159918823\n",
      "Val Epoch Loss: 0.01624992792188239\n",
      "Training Epoch : [421/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013616125270920364\n",
      "Val Epoch Loss: 0.01647733512493831\n",
      "Training Epoch : [422/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012108608099975084\n",
      "Val Epoch Loss: 0.016964529343807187\n",
      "Training Epoch : [423/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012143572057156186\n",
      "Val Epoch Loss: 0.016090055607492104\n",
      "Training Epoch : [424/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013006721574224923\n",
      "Val Epoch Loss: 0.013348431211565375\n",
      "Training Epoch : [425/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01229181584264887\n",
      "Val Epoch Loss: 0.01506994047055119\n",
      "Training Epoch : [426/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012299453913185158\n",
      "Val Epoch Loss: 0.015177317083151521\n",
      "Training Epoch : [427/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013655853996935644\n",
      "Val Epoch Loss: 0.013347139591228609\n",
      "Training Epoch : [428/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012212297548294851\n",
      "Val Epoch Loss: 0.015349015659395311\n",
      "Training Epoch : [429/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013539760250990329\n",
      "Val Epoch Loss: 0.015949796349128808\n",
      "Training Epoch : [430/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013027958571910858\n",
      "Val Epoch Loss: 0.015522904923847435\n",
      "Training Epoch : [431/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013434670081264094\n",
      "Val Epoch Loss: 0.0169473297091348\n",
      "Training Epoch : [432/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012945903906304585\n",
      "Val Epoch Loss: 0.012672047161399141\n",
      "Training Epoch : [433/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012568613024134385\n",
      "Val Epoch Loss: 0.013204323948900165\n",
      "Training Epoch : [434/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012424647391430642\n",
      "Val Epoch Loss: 0.016994559956640985\n",
      "Training Epoch : [435/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014346417676853506\n",
      "Val Epoch Loss: 0.013553591082056397\n",
      "Training Epoch : [436/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015101119973941854\n",
      "Val Epoch Loss: 0.014818176905943179\n",
      "Training Epoch : [437/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014115341914523589\n",
      "Val Epoch Loss: 0.015453430306239935\n",
      "Training Epoch : [438/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013071231617543259\n",
      "Val Epoch Loss: 0.02106231262319182\n",
      "Training Epoch : [439/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014421722802676652\n",
      "Val Epoch Loss: 0.02613028649877953\n",
      "Training Epoch : [440/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015570673331814376\n",
      "Val Epoch Loss: 0.01729991672389013\n",
      "Training Epoch : [441/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012895730348598016\n",
      "Val Epoch Loss: 0.01908806093515044\n",
      "Training Epoch : [442/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012840683081824528\n",
      "Val Epoch Loss: 0.013645547046938813\n",
      "Training Epoch : [443/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013886157741868183\n",
      "Val Epoch Loss: 0.012796826069365795\n",
      "Training Epoch : [444/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012405836047898782\n",
      "Val Epoch Loss: 0.017489364977288795\n",
      "Training Epoch : [445/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012811956804637847\n",
      "Val Epoch Loss: 0.014427797663589254\n",
      "Training Epoch : [446/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012598700845908178\n",
      "Val Epoch Loss: 0.019727628466697705\n",
      "Training Epoch : [447/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012793529092481262\n",
      "Val Epoch Loss: 0.015178345928066656\n",
      "Training Epoch : [448/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012603624503275282\n",
      "Val Epoch Loss: 0.014948750210399004\n",
      "Training Epoch : [449/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013331671098345205\n",
      "Val Epoch Loss: 0.021958990641379433\n",
      "Training Epoch : [450/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013499391191688023\n",
      "Val Epoch Loss: 0.016701476311458175\n",
      "Training Epoch : [451/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013278636958842216\n",
      "Val Epoch Loss: 0.01633320106989949\n",
      "Training Epoch : [452/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01314674533511463\n",
      "Val Epoch Loss: 0.0168614006650291\n",
      "Training Epoch : [453/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012618887532306345\n",
      "Val Epoch Loss: 0.016283107065215807\n",
      "Training Epoch : [454/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012284104859358385\n",
      "Val Epoch Loss: 0.016621260379906744\n",
      "Training Epoch : [455/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011570662750225318\n",
      "Val Epoch Loss: 0.01693846755446621\n",
      "Training Epoch : [456/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013494305484192936\n",
      "Val Epoch Loss: 0.014403900791780631\n",
      "Training Epoch : [457/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013143939039620915\n",
      "Val Epoch Loss: 0.012730057928772447\n",
      "Training Epoch : [458/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01266114921063969\n",
      "Val Epoch Loss: 0.01444902328320926\n",
      "Training Epoch : [459/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014113819765809336\n",
      "Val Epoch Loss: 0.019334029283766683\n",
      "Training Epoch : [460/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012028707804060295\n",
      "Val Epoch Loss: 0.01537525564076771\n",
      "Training Epoch : [461/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014709710756218746\n",
      "Val Epoch Loss: 0.01459660773214541\n",
      "Training Epoch : [462/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013252627717233017\n",
      "Val Epoch Loss: 0.018574238247789542\n",
      "Training Epoch : [463/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012856824421568921\n",
      "Val Epoch Loss: 0.017652399462684498\n",
      "Training Epoch : [464/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014117566844154345\n",
      "Val Epoch Loss: 0.01774569961401683\n",
      "Training Epoch : [465/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013683598905213569\n",
      "Val Epoch Loss: 0.0176808474061545\n",
      "Training Epoch : [466/1000]===============================================================================\n",
      "Training Epoch Loss: 0.0145054243652052\n",
      "Val Epoch Loss: 0.014566737729565878\n",
      "Training Epoch : [467/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01536380234909685\n",
      "Val Epoch Loss: 0.01865196654475049\n",
      "Training Epoch : [468/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014695342188995136\n",
      "Val Epoch Loss: 0.015022931246724176\n",
      "Training Epoch : [469/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012549957726150751\n",
      "Val Epoch Loss: 0.013814134406857193\n",
      "Training Epoch : [470/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012751419783422821\n",
      "Val Epoch Loss: 0.022731273171589954\n",
      "Training Epoch : [471/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013792673419964941\n",
      "Val Epoch Loss: 0.020223735142376666\n",
      "Training Epoch : [472/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014501134619901055\n",
      "Val Epoch Loss: 0.013711862606118973\n",
      "Training Epoch : [473/1000]===============================================================================\n",
      "Training Epoch Loss: 0.0178624531989427\n",
      "Val Epoch Loss: 0.019123806749887177\n",
      "Training Epoch : [474/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015276727585220024\n",
      "Val Epoch Loss: 0.016052835375854845\n",
      "Training Epoch : [475/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012538854547433163\n",
      "Val Epoch Loss: 0.015109728182371902\n",
      "Training Epoch : [476/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012637264683450522\n",
      "Val Epoch Loss: 0.01853385706750774\n",
      "Training Epoch : [477/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012772143303759788\n",
      "Val Epoch Loss: 0.019858558325244014\n",
      "Training Epoch : [478/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01220482732414415\n",
      "Val Epoch Loss: 0.016774505709212174\n",
      "Training Epoch : [479/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012761883364108047\n",
      "Val Epoch Loss: 0.015195394192185057\n",
      "Training Epoch : [480/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012667065167701557\n",
      "Val Epoch Loss: 0.014349038489040379\n",
      "Training Epoch : [481/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012366404291242361\n",
      "Val Epoch Loss: 0.014356821836707624\n",
      "Training Epoch : [482/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011932365081616138\n",
      "Val Epoch Loss: 0.016795172708378613\n",
      "Training Epoch : [483/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01387756065416493\n",
      "Val Epoch Loss: 0.015112065510995882\n",
      "Training Epoch : [484/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01418556991082273\n",
      "Val Epoch Loss: 0.018224824324103172\n",
      "Training Epoch : [485/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01391528766161125\n",
      "Val Epoch Loss: 0.01473528880466658\n",
      "Training Epoch : [486/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013469805695900791\n",
      "Val Epoch Loss: 0.019340358233373416\n",
      "Training Epoch : [487/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01306083661160971\n",
      "Val Epoch Loss: 0.014788626451167818\n",
      "Training Epoch : [488/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012201179407144847\n",
      "Val Epoch Loss: 0.014668955440069303\n",
      "Training Epoch : [489/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01361547506071235\n",
      "Val Epoch Loss: 0.015108428103542937\n",
      "Training Epoch : [490/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013245299483012212\n",
      "Val Epoch Loss: 0.016228805872072514\n",
      "Training Epoch : [491/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013084859366675741\n",
      "Val Epoch Loss: 0.01715913512223204\n",
      "Training Epoch : [492/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012118348484172634\n",
      "Val Epoch Loss: 0.013086657647938026\n",
      "Training Epoch : [493/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012999389443154397\n",
      "Val Epoch Loss: 0.01683177707385958\n",
      "Training Epoch : [494/1000]===============================================================================\n",
      "Training Epoch Loss: 0.0124183996874643\n",
      "Val Epoch Loss: 0.017479557543992996\n",
      "Training Epoch : [495/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013647200539708138\n",
      "Val Epoch Loss: 0.015806470293012496\n",
      "Training Epoch : [496/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012528486623379746\n",
      "Val Epoch Loss: 0.012444094932442041\n",
      "Training Epoch : [497/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012135925384140328\n",
      "Val Epoch Loss: 0.015089340768386856\n",
      "Training Epoch : [498/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013036809543049649\n",
      "Val Epoch Loss: 0.018011233989933605\n",
      "Training Epoch : [499/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013417338878896675\n",
      "Val Epoch Loss: 0.013869604059053879\n",
      "Training Epoch : [500/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013005095877145467\n",
      "Val Epoch Loss: 0.013303542120602765\n",
      "Training Epoch : [501/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01373858803785161\n",
      "Val Epoch Loss: 0.016228648228877176\n",
      "Training Epoch : [502/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013578923664202816\n",
      "Val Epoch Loss: 0.014454436965599214\n",
      "Training Epoch : [503/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012537044007331133\n",
      "Val Epoch Loss: 0.012239850737387314\n",
      "Training Epoch : [504/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012106925028523332\n",
      "Val Epoch Loss: 0.01604275873442499\n",
      "Training Epoch : [505/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012957155557447359\n",
      "Val Epoch Loss: 0.01380004602680473\n",
      "Training Epoch : [506/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01288878726527879\n",
      "Val Epoch Loss: 0.013759217064178205\n",
      "Training Epoch : [507/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012540483847260475\n",
      "Val Epoch Loss: 0.014764511962359967\n",
      "Training Epoch : [508/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012106257588847688\n",
      "Val Epoch Loss: 0.01574035176829631\n",
      "Training Epoch : [509/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012477951271361426\n",
      "Val Epoch Loss: 0.02328607066240358\n",
      "Training Epoch : [510/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016032758046333726\n",
      "Val Epoch Loss: 0.013838758679891103\n",
      "Training Epoch : [511/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013968339533005891\n",
      "Val Epoch Loss: 0.01614766218393159\n",
      "Training Epoch : [512/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012986442980993735\n",
      "Val Epoch Loss: 0.015817347576222874\n",
      "Training Epoch : [513/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013167045275239568\n",
      "Val Epoch Loss: 0.012399607471276173\n",
      "Training Epoch : [514/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011934907445193906\n",
      "Val Epoch Loss: 0.014750628004549071\n",
      "Training Epoch : [515/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014598149928803506\n",
      "Val Epoch Loss: 0.014284036149834528\n",
      "Training Epoch : [516/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013838159190000672\n",
      "Val Epoch Loss: 0.0149348534790701\n",
      "Training Epoch : [517/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014974871356236307\n",
      "Val Epoch Loss: 0.01693271566182375\n",
      "Training Epoch : [518/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012326287914459643\n",
      "Val Epoch Loss: 0.0162791448559514\n",
      "Training Epoch : [519/1000]===============================================================================\n",
      "Training Epoch Loss: 0.0137827274340548\n",
      "Val Epoch Loss: 0.01592758130649791\n",
      "Training Epoch : [520/1000]===============================================================================\n",
      "Training Epoch Loss: 0.019944171053602508\n",
      "Val Epoch Loss: 0.01320147400991501\n",
      "Training Epoch : [521/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014124473752944093\n",
      "Val Epoch Loss: 0.017818631248940762\n",
      "Training Epoch : [522/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014513788695790265\n",
      "Val Epoch Loss: 0.012928995502941115\n",
      "Training Epoch : [523/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01293476680783849\n",
      "Val Epoch Loss: 0.013504045358307562\n",
      "Training Epoch : [524/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012866363983209195\n",
      "Val Epoch Loss: 0.015170305587449357\n",
      "Training Epoch : [525/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013578106330609635\n",
      "Val Epoch Loss: 0.013928266245210054\n",
      "Training Epoch : [526/1000]===============================================================================\n",
      "Training Epoch Loss: 0.017787565477192402\n",
      "Val Epoch Loss: 0.013654651343945022\n",
      "Training Epoch : [527/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014313737567710249\n",
      "Val Epoch Loss: 0.017135403707231347\n",
      "Training Epoch : [528/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01380767327684321\n",
      "Val Epoch Loss: 0.01535768557560483\n",
      "Training Epoch : [529/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012696761956536457\n",
      "Val Epoch Loss: 0.018753039630705882\n",
      "Training Epoch : [530/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012826264125147932\n",
      "Val Epoch Loss: 0.014234006495251762\n",
      "Training Epoch : [531/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01293864008039236\n",
      "Val Epoch Loss: 0.01930006515651353\n",
      "Training Epoch : [532/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013034280217987927\n",
      "Val Epoch Loss: 0.015465501156684599\n",
      "Training Epoch : [533/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011373560581552354\n",
      "Val Epoch Loss: 0.014664170812466182\n",
      "Training Epoch : [534/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012240538153013117\n",
      "Val Epoch Loss: 0.017119043797720224\n",
      "Training Epoch : [535/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012044279799355488\n",
      "Val Epoch Loss: 0.013080131961032748\n",
      "Training Epoch : [536/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011786665375295439\n",
      "Val Epoch Loss: 0.013328391208472712\n",
      "Training Epoch : [537/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011330095291333763\n",
      "Val Epoch Loss: 0.01277871161399066\n",
      "Training Epoch : [538/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012080909218639135\n",
      "Val Epoch Loss: 0.01470947106932535\n",
      "Training Epoch : [539/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013495915740924446\n",
      "Val Epoch Loss: 0.014924175631389707\n",
      "Training Epoch : [540/1000]===============================================================================\n",
      "Training Epoch Loss: 0.0137120492168163\n",
      "Val Epoch Loss: 0.018351123037159835\n",
      "Training Epoch : [541/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013757089471542522\n",
      "Val Epoch Loss: 0.02091816477753271\n",
      "Training Epoch : [542/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015075469781693659\n",
      "Val Epoch Loss: 0.016271046650792032\n",
      "Training Epoch : [543/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012198150844166153\n",
      "Val Epoch Loss: 0.015682654862757772\n",
      "Training Epoch : [544/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01348246148738422\n",
      "Val Epoch Loss: 0.01494585555108068\n",
      "Training Epoch : [545/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012061280124869785\n",
      "Val Epoch Loss: 0.020747192253015544\n",
      "Training Epoch : [546/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012538550593154995\n",
      "Val Epoch Loss: 0.01304965666464628\n",
      "Training Epoch : [547/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013412813018811377\n",
      "Val Epoch Loss: 0.01449284204404409\n",
      "Training Epoch : [548/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012747664200632195\n",
      "Val Epoch Loss: 0.016038123668343025\n",
      "Training Epoch : [549/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014668864737215796\n",
      "Val Epoch Loss: 0.015799613338650056\n",
      "Training Epoch : [550/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014003790032706763\n",
      "Val Epoch Loss: 0.015021007985955006\n",
      "Training Epoch : [551/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011318339022660726\n",
      "Val Epoch Loss: 0.013607304215455722\n",
      "Training Epoch : [552/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012037442801030059\n",
      "Val Epoch Loss: 0.012438299690410005\n",
      "Training Epoch : [553/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01226819938931026\n",
      "Val Epoch Loss: 0.015541500811722423\n",
      "Training Epoch : [554/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011374974843898886\n",
      "Val Epoch Loss: 0.012739059147670082\n",
      "Training Epoch : [555/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013102771444736342\n",
      "Val Epoch Loss: 0.016637648339383304\n",
      "Training Epoch : [556/1000]===============================================================================\n",
      "Training Epoch Loss: 0.0121388331447777\n",
      "Val Epoch Loss: 0.012214956210779124\n",
      "Training Epoch : [557/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013143492862582207\n",
      "Val Epoch Loss: 0.02034899853415003\n",
      "Training Epoch : [558/1000]===============================================================================\n",
      "Training Epoch Loss: 0.0125119550349681\n",
      "Val Epoch Loss: 0.013286514503354403\n",
      "Training Epoch : [559/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011966356909588763\n",
      "Val Epoch Loss: 0.013308328800025936\n",
      "Training Epoch : [560/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013012584078272707\n",
      "Val Epoch Loss: 0.015677352169357044\n",
      "Training Epoch : [561/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011485635989198559\n",
      "Val Epoch Loss: 0.012536422289773136\n",
      "Training Epoch : [562/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013610642869025469\n",
      "Val Epoch Loss: 0.015331729078561215\n",
      "Training Epoch : [563/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01182331924179667\n",
      "Val Epoch Loss: 0.012045384639733177\n",
      "Training Epoch : [564/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013447086596371312\n",
      "Val Epoch Loss: 0.016702329727674003\n",
      "Training Epoch : [565/1000]===============================================================================\n",
      "Training Epoch Loss: 0.0133225094821108\n",
      "Val Epoch Loss: 0.015433482277378636\n",
      "Training Epoch : [566/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013129131647905237\n",
      "Val Epoch Loss: 0.015613442489044055\n",
      "Training Epoch : [567/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012521441878848955\n",
      "Val Epoch Loss: 0.01636927447428829\n",
      "Training Epoch : [568/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012039057885933863\n",
      "Val Epoch Loss: 0.013813771923244195\n",
      "Training Epoch : [569/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011526145420870498\n",
      "Val Epoch Loss: 0.013236797089216691\n",
      "Training Epoch : [570/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011528208361644494\n",
      "Val Epoch Loss: 0.0131823160091268\n",
      "Training Epoch : [571/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014048297914039148\n",
      "Val Epoch Loss: 0.01899140138993971\n",
      "Training Epoch : [572/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015078055819398478\n",
      "Val Epoch Loss: 0.014276437253054036\n",
      "Training Epoch : [573/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014206509301929097\n",
      "Val Epoch Loss: 0.01416934290931462\n",
      "Training Epoch : [574/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013969110251453361\n",
      "Val Epoch Loss: 0.016367238673602083\n",
      "Training Epoch : [575/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013629699370970852\n",
      "Val Epoch Loss: 0.016823266172554883\n",
      "Training Epoch : [576/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012338203990733937\n",
      "Val Epoch Loss: 0.014001492285292204\n",
      "Training Epoch : [577/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01233318886768661\n",
      "Val Epoch Loss: 0.014258573862638227\n",
      "Training Epoch : [578/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012465419713407755\n",
      "Val Epoch Loss: 0.012477725684507996\n",
      "Training Epoch : [579/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01244935721747185\n",
      "Val Epoch Loss: 0.013218139243116113\n",
      "Training Epoch : [580/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012831693686740963\n",
      "Val Epoch Loss: 0.012181685324513206\n",
      "Training Epoch : [581/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01197022677546269\n",
      "Val Epoch Loss: 0.015032012168759186\n",
      "Training Epoch : [582/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011825074882883774\n",
      "Val Epoch Loss: 0.015423457420087959\n",
      "Training Epoch : [583/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011460086698398778\n",
      "Val Epoch Loss: 0.013208044870560499\n",
      "Training Epoch : [584/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012330058275869018\n",
      "Val Epoch Loss: 0.01348779180185183\n",
      "Training Epoch : [585/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012280903781126989\n",
      "Val Epoch Loss: 0.015558105596267668\n",
      "Training Epoch : [586/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01182653830925885\n",
      "Val Epoch Loss: 0.01873103236338418\n",
      "Training Epoch : [587/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013325141620283065\n",
      "Val Epoch Loss: 0.016044394911181082\n",
      "Training Epoch : [588/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01207732600405028\n",
      "Val Epoch Loss: 0.012263454521368993\n",
      "Training Epoch : [589/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013137254839468943\n",
      "Val Epoch Loss: 0.016190735022782496\n",
      "Training Epoch : [590/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012960094419357023\n",
      "Val Epoch Loss: 0.012475435695561924\n",
      "Training Epoch : [591/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012073110592992682\n",
      "Val Epoch Loss: 0.012504073345457743\n",
      "Training Epoch : [592/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013329439673964915\n",
      "Val Epoch Loss: 0.01425465726673505\n",
      "Training Epoch : [593/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014220758022642449\n",
      "Val Epoch Loss: 0.014891090318805686\n",
      "Training Epoch : [594/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012132780283297362\n",
      "Val Epoch Loss: 0.014740671895867164\n",
      "Training Epoch : [595/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011679499115990965\n",
      "Val Epoch Loss: 0.0138438753063766\n",
      "Training Epoch : [596/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012004035598549404\n",
      "Val Epoch Loss: 0.012755303553268803\n",
      "Training Epoch : [597/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012572796647682002\n",
      "Val Epoch Loss: 0.015281486962186662\n",
      "Training Epoch : [598/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015501455085254029\n",
      "Val Epoch Loss: 0.016784167565148028\n",
      "Training Epoch : [599/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012662803778718961\n",
      "Val Epoch Loss: 0.01394988386125892\n",
      "Training Epoch : [600/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012096501617251258\n",
      "Val Epoch Loss: 0.019365953802327186\n",
      "Training Epoch : [601/1000]===============================================================================\n",
      "Training Epoch Loss: 0.017125275255622047\n",
      "Val Epoch Loss: 0.01653699933453218\n",
      "Training Epoch : [602/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013374011669504015\n",
      "Val Epoch Loss: 0.01327051574940254\n",
      "Training Epoch : [603/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012208644006597368\n",
      "Val Epoch Loss: 0.016579230150550996\n",
      "Training Epoch : [604/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01177746714337876\n",
      "Val Epoch Loss: 0.014538454536752971\n",
      "Training Epoch : [605/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011832170874664658\n",
      "Val Epoch Loss: 0.017987076361590113\n",
      "Training Epoch : [606/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01190234384940643\n",
      "Val Epoch Loss: 0.012628224698250722\n",
      "Training Epoch : [607/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011891847280295272\n",
      "Val Epoch Loss: 0.013630306049507405\n",
      "Training Epoch : [608/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013369643962696978\n",
      "Val Epoch Loss: 0.015726360210560654\n",
      "Training Epoch : [609/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012376311553740188\n",
      "Val Epoch Loss: 0.01593386786331202\n",
      "Training Epoch : [610/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011700190726275506\n",
      "Val Epoch Loss: 0.01651216011439254\n",
      "Training Epoch : [611/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012657373824990109\n",
      "Val Epoch Loss: 0.01421570066993713\n",
      "Training Epoch : [612/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012722548538524853\n",
      "Val Epoch Loss: 0.024051720588958186\n",
      "Training Epoch : [613/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01289874986794434\n",
      "Val Epoch Loss: 0.014308464645996298\n",
      "Training Epoch : [614/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012344661291296544\n",
      "Val Epoch Loss: 0.016272776343516614\n",
      "Training Epoch : [615/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011460094835217061\n",
      "Val Epoch Loss: 0.011634859621010133\n",
      "Saving Best Model =======================================>\n",
      "Training Epoch : [616/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01159039031910269\n",
      "Val Epoch Loss: 0.015570816050871815\n",
      "Training Epoch : [617/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012251449479280333\n",
      "Val Epoch Loss: 0.01525547973486889\n",
      "Training Epoch : [618/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012453193065563315\n",
      "Val Epoch Loss: 0.013012111548481411\n",
      "Training Epoch : [619/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01310217370720286\n",
      "Val Epoch Loss: 0.014305663376274569\n",
      "Training Epoch : [620/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013230294087215475\n",
      "Val Epoch Loss: 0.013697873741253525\n",
      "Training Epoch : [621/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011454022268911726\n",
      "Val Epoch Loss: 0.014010410657072881\n",
      "Training Epoch : [622/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011581285096901027\n",
      "Val Epoch Loss: 0.013835772213305494\n",
      "Training Epoch : [623/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013090203026015507\n",
      "Val Epoch Loss: 0.011854672960716447\n",
      "Training Epoch : [624/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012712439203536824\n",
      "Val Epoch Loss: 0.012898364557737583\n",
      "Training Epoch : [625/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011861843547146571\n",
      "Val Epoch Loss: 0.014317834992605066\n",
      "Training Epoch : [626/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012963123521522471\n",
      "Val Epoch Loss: 0.013249455362950502\n",
      "Training Epoch : [627/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01301949331536889\n",
      "Val Epoch Loss: 0.013259859789334433\n",
      "Training Epoch : [628/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011784022539167813\n",
      "Val Epoch Loss: 0.014049110134548851\n",
      "Training Epoch : [629/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014037088809632942\n",
      "Val Epoch Loss: 0.012346491914573371\n",
      "Training Epoch : [630/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012210133681563954\n",
      "Val Epoch Loss: 0.013585218672616113\n",
      "Training Epoch : [631/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012232802048521606\n",
      "Val Epoch Loss: 0.011861830231378247\n",
      "Training Epoch : [632/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012704005023758663\n",
      "Val Epoch Loss: 0.014073347291406734\n",
      "Training Epoch : [633/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012684425829272521\n",
      "Val Epoch Loss: 0.01888206094123521\n",
      "Training Epoch : [634/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013543964388143075\n",
      "Val Epoch Loss: 0.013986631311892875\n",
      "Training Epoch : [635/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012172131308991658\n",
      "Val Epoch Loss: 0.015017706782887935\n",
      "Training Epoch : [636/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011927800302050616\n",
      "Val Epoch Loss: 0.01279132788859945\n",
      "Training Epoch : [637/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012392635481726182\n",
      "Val Epoch Loss: 0.012571265553369335\n",
      "Training Epoch : [638/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011622881639356675\n",
      "Val Epoch Loss: 0.0128467492897088\n",
      "Training Epoch : [639/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011765876993242847\n",
      "Val Epoch Loss: 0.014569709245973316\n",
      "Training Epoch : [640/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011672363444966706\n",
      "Val Epoch Loss: 0.011776635720468076\n",
      "Training Epoch : [641/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011685398125432823\n",
      "Val Epoch Loss: 0.014766095278135157\n",
      "Training Epoch : [642/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01257882054012857\n",
      "Val Epoch Loss: 0.014322831951032736\n",
      "Training Epoch : [643/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013163418484557616\n",
      "Val Epoch Loss: 0.013751213408173308\n",
      "Training Epoch : [644/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011709212350021852\n",
      "Val Epoch Loss: 0.01169802652714823\n",
      "Training Epoch : [645/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01107316601433252\n",
      "Val Epoch Loss: 0.014748353396977396\n",
      "Training Epoch : [646/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011347855980459013\n",
      "Val Epoch Loss: 0.013535588686385094\n",
      "Training Epoch : [647/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01122178583356895\n",
      "Val Epoch Loss: 0.013730670906768768\n",
      "Training Epoch : [648/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01136430583306049\n",
      "Val Epoch Loss: 0.014369228755975584\n",
      "Training Epoch : [649/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011884688914410378\n",
      "Val Epoch Loss: 0.01194887309132977\n",
      "Training Epoch : [650/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012303024824512633\n",
      "Val Epoch Loss: 0.01652874712721984\n",
      "Training Epoch : [651/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01443196536580983\n",
      "Val Epoch Loss: 0.019772638542283522\n",
      "Training Epoch : [652/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015061072662080588\n",
      "Val Epoch Loss: 0.014933786416230234\n",
      "Training Epoch : [653/1000]===============================================================================\n",
      "Training Epoch Loss: 0.017227598392453632\n",
      "Val Epoch Loss: 0.01280704110333892\n",
      "Training Epoch : [654/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013983171408701884\n",
      "Val Epoch Loss: 0.012364509772803439\n",
      "Training Epoch : [655/1000]===============================================================================\n",
      "Training Epoch Loss: 0.0129358376082229\n",
      "Val Epoch Loss: 0.013266009332207767\n",
      "Training Epoch : [656/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012616371074201245\n",
      "Val Epoch Loss: 0.014849343504082705\n",
      "Training Epoch : [657/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012617576279138265\n",
      "Val Epoch Loss: 0.013145806831552795\n",
      "Training Epoch : [658/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011743478820120034\n",
      "Val Epoch Loss: 0.014482429312346013\n",
      "Training Epoch : [659/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01173554548356486\n",
      "Val Epoch Loss: 0.011389665041382327\n",
      "Saving Best Model =======================================>\n",
      "Training Epoch : [660/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012069261828927617\n",
      "Val Epoch Loss: 0.012351919450003678\n",
      "Training Epoch : [661/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011511901516075196\n",
      "Val Epoch Loss: 0.013439584528034749\n",
      "Training Epoch : [662/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011615433301286478\n",
      "Val Epoch Loss: 0.011748805542562255\n",
      "Training Epoch : [663/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012119149495112268\n",
      "Val Epoch Loss: 0.012610619575226386\n",
      "Training Epoch : [664/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011697919181499043\n",
      "Val Epoch Loss: 0.012289787468034774\n",
      "Training Epoch : [665/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014010210742095583\n",
      "Val Epoch Loss: 0.0318339651107396\n",
      "Training Epoch : [666/1000]===============================================================================\n",
      "Training Epoch Loss: 0.022862241186789777\n",
      "Val Epoch Loss: 0.01327728063456322\n",
      "Training Epoch : [667/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014229671416902229\n",
      "Val Epoch Loss: 0.017134255222596328\n",
      "Training Epoch : [668/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014292630661082896\n",
      "Val Epoch Loss: 0.013518955270563694\n",
      "Training Epoch : [669/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01397851023724989\n",
      "Val Epoch Loss: 0.015118502166220233\n",
      "Training Epoch : [670/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012874122856086806\n",
      "Val Epoch Loss: 0.015447481256591058\n",
      "Training Epoch : [671/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013745484947177925\n",
      "Val Epoch Loss: 0.012544594535077187\n",
      "Training Epoch : [672/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011877589540458039\n",
      "Val Epoch Loss: 0.014177845178270027\n",
      "Training Epoch : [673/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012006795646524742\n",
      "Val Epoch Loss: 0.014449389087722489\n",
      "Training Epoch : [674/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012233057206398562\n",
      "Val Epoch Loss: 0.014099100289106565\n",
      "Training Epoch : [675/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012441754267600021\n",
      "Val Epoch Loss: 0.01508419308198714\n",
      "Training Epoch : [676/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011433047855174855\n",
      "Val Epoch Loss: 0.01251961874597902\n",
      "Training Epoch : [677/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011579705916933324\n",
      "Val Epoch Loss: 0.01384808875369153\n",
      "Training Epoch : [678/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01158557775871534\n",
      "Val Epoch Loss: 0.012486200166198327\n",
      "Training Epoch : [679/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011664977382966563\n",
      "Val Epoch Loss: 0.012888965147890542\n",
      "Training Epoch : [680/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01190719863791999\n",
      "Val Epoch Loss: 0.012456216356153354\n",
      "Training Epoch : [681/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011435383318090126\n",
      "Val Epoch Loss: 0.014075151600225485\n",
      "Training Epoch : [682/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011042170986337098\n",
      "Val Epoch Loss: 0.011611753114266321\n",
      "Training Epoch : [683/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011699623550827565\n",
      "Val Epoch Loss: 0.01289717211559611\n",
      "Training Epoch : [684/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011289270080037807\n",
      "Val Epoch Loss: 0.013258333056000993\n",
      "Training Epoch : [685/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012013488571698728\n",
      "Val Epoch Loss: 0.014621031995997518\n",
      "Training Epoch : [686/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013114831140755038\n",
      "Val Epoch Loss: 0.011484919899406745\n",
      "Training Epoch : [687/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011951031334894268\n",
      "Val Epoch Loss: 0.01302458905143124\n",
      "Training Epoch : [688/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011228833788711774\n",
      "Val Epoch Loss: 0.012041597010561657\n",
      "Training Epoch : [689/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01150122086370462\n",
      "Val Epoch Loss: 0.011834222346231783\n",
      "Training Epoch : [690/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012753535821837815\n",
      "Val Epoch Loss: 0.017313451294811738\n",
      "Training Epoch : [691/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012670463637301796\n",
      "Val Epoch Loss: 0.012767532880963316\n",
      "Training Epoch : [692/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012301078924026928\n",
      "Val Epoch Loss: 0.012611647722415478\n",
      "Training Epoch : [693/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013596655139209409\n",
      "Val Epoch Loss: 0.0149404033589618\n",
      "Training Epoch : [694/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012195416407561615\n",
      "Val Epoch Loss: 0.012281830936492617\n",
      "Training Epoch : [695/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01229148527215186\n",
      "Val Epoch Loss: 0.01159178771501358\n",
      "Training Epoch : [696/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012338817658785143\n",
      "Val Epoch Loss: 0.013748126784584633\n",
      "Training Epoch : [697/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012067209537092009\n",
      "Val Epoch Loss: 0.016448537512731395\n",
      "Training Epoch : [698/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012196861256502177\n",
      "Val Epoch Loss: 0.014285029907784375\n",
      "Training Epoch : [699/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011526895858543483\n",
      "Val Epoch Loss: 0.012392786104771259\n",
      "Training Epoch : [700/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013309182427627476\n",
      "Val Epoch Loss: 0.013352370833193785\n",
      "Training Epoch : [701/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01230561642564441\n",
      "Val Epoch Loss: 0.012315569531270549\n",
      "Training Epoch : [702/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012140429774789433\n",
      "Val Epoch Loss: 0.012104988783616327\n",
      "Training Epoch : [703/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011720017824125918\n",
      "Val Epoch Loss: 0.011627717049314493\n",
      "Training Epoch : [704/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010951914959342071\n",
      "Val Epoch Loss: 0.01230358792542431\n",
      "Training Epoch : [705/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011022316218402824\n",
      "Val Epoch Loss: 0.01213628966300013\n",
      "Training Epoch : [706/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011933006248191783\n",
      "Val Epoch Loss: 0.01662566355049708\n",
      "Training Epoch : [707/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014271838835587627\n",
      "Val Epoch Loss: 0.013475529267452657\n",
      "Training Epoch : [708/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011951992214706383\n",
      "Val Epoch Loss: 0.016317955950446623\n",
      "Training Epoch : [709/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012047224187929379\n",
      "Val Epoch Loss: 0.015810940514232864\n",
      "Training Epoch : [710/1000]===============================================================================\n",
      "Training Epoch Loss: 0.016226020604862196\n",
      "Val Epoch Loss: 0.016578070647818476\n",
      "Training Epoch : [711/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01297525745375376\n",
      "Val Epoch Loss: 0.012015717722916682\n",
      "Training Epoch : [712/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012180227810811056\n",
      "Val Epoch Loss: 0.014018451660185269\n",
      "Training Epoch : [713/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01250981451257279\n",
      "Val Epoch Loss: 0.019480365077278725\n",
      "Training Epoch : [714/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012167449967053375\n",
      "Val Epoch Loss: 0.012205629540595078\n",
      "Training Epoch : [715/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011364579813456848\n",
      "Val Epoch Loss: 0.013567134721118859\n",
      "Training Epoch : [716/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011838701284049373\n",
      "Val Epoch Loss: 0.013702130139695089\n",
      "Training Epoch : [717/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010931884894441617\n",
      "Val Epoch Loss: 0.011545432443534466\n",
      "Training Epoch : [718/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011092205605420628\n",
      "Val Epoch Loss: 0.014327469226708146\n",
      "Training Epoch : [719/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011471519295714404\n",
      "Val Epoch Loss: 0.012971221267147675\n",
      "Training Epoch : [720/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011983247277768035\n",
      "Val Epoch Loss: 0.012399748670807304\n",
      "Training Epoch : [721/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011658506467938423\n",
      "Val Epoch Loss: 0.012551624670250979\n",
      "Training Epoch : [722/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012071029577208193\n",
      "Val Epoch Loss: 0.01358024196815677\n",
      "Training Epoch : [723/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012739916993795257\n",
      "Val Epoch Loss: 0.013886074164857794\n",
      "Training Epoch : [724/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011455406777952848\n",
      "Val Epoch Loss: 0.012711204822775662\n",
      "Training Epoch : [725/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011177224634019168\n",
      "Val Epoch Loss: 0.011731744912379471\n",
      "Training Epoch : [726/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011334151029586792\n",
      "Val Epoch Loss: 0.0131511208203033\n",
      "Training Epoch : [727/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012109769814598718\n",
      "Val Epoch Loss: 0.014317548576448309\n",
      "Training Epoch : [728/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011798536184390909\n",
      "Val Epoch Loss: 0.0132141192903203\n",
      "Training Epoch : [729/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011930340337321946\n",
      "Val Epoch Loss: 0.01556857862957067\n",
      "Training Epoch : [730/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012851943139378963\n",
      "Val Epoch Loss: 0.011987592888640632\n",
      "Training Epoch : [731/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01207169471308589\n",
      "Val Epoch Loss: 0.012573532458382511\n",
      "Training Epoch : [732/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012106859688892177\n",
      "Val Epoch Loss: 0.012465925199457592\n",
      "Training Epoch : [733/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013095371915321601\n",
      "Val Epoch Loss: 0.01446212365561606\n",
      "Training Epoch : [734/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012001494656463987\n",
      "Val Epoch Loss: 0.01242551220179609\n",
      "Training Epoch : [735/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011236132980373344\n",
      "Val Epoch Loss: 0.011702926121774669\n",
      "Training Epoch : [736/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012287695867646681\n",
      "Val Epoch Loss: 0.01300991341275604\n",
      "Training Epoch : [737/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011998727819637248\n",
      "Val Epoch Loss: 0.012044474928284456\n",
      "Training Epoch : [738/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012752770838376722\n",
      "Val Epoch Loss: 0.011968329904208841\n",
      "Training Epoch : [739/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011705856035022359\n",
      "Val Epoch Loss: 0.011330881146070743\n",
      "Saving Best Model =======================================>\n",
      "Training Epoch : [740/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01149074082556916\n",
      "Val Epoch Loss: 0.012843857384531907\n",
      "Training Epoch : [741/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011248040674744468\n",
      "Val Epoch Loss: 0.012697363704271419\n",
      "Training Epoch : [742/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011639316345712072\n",
      "Val Epoch Loss: 0.01417366392132336\n",
      "Training Epoch : [743/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011856021175749208\n",
      "Val Epoch Loss: 0.011834057360446374\n",
      "Training Epoch : [744/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011268205744655509\n",
      "Val Epoch Loss: 0.01231283019450003\n",
      "Training Epoch : [745/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011502038810010018\n",
      "Val Epoch Loss: 0.012273656965637775\n",
      "Training Epoch : [746/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011447355077650986\n",
      "Val Epoch Loss: 0.012353736843674835\n",
      "Training Epoch : [747/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014179920679644534\n",
      "Val Epoch Loss: 0.02150457900777263\n",
      "Training Epoch : [748/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012801691007457282\n",
      "Val Epoch Loss: 0.013290999457240105\n",
      "Training Epoch : [749/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013416208816986335\n",
      "Val Epoch Loss: 0.01411406885477175\n",
      "Training Epoch : [750/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012532960280383887\n",
      "Val Epoch Loss: 0.013899986650549659\n",
      "Training Epoch : [751/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012374281098968104\n",
      "Val Epoch Loss: 0.012390870406402667\n",
      "Training Epoch : [752/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011799647399273357\n",
      "Val Epoch Loss: 0.015310138173701585\n",
      "Training Epoch : [753/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01231306627098667\n",
      "Val Epoch Loss: 0.014578443896760674\n",
      "Training Epoch : [754/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013454856056916086\n",
      "Val Epoch Loss: 0.014276969773498805\n",
      "Training Epoch : [755/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013188146814507874\n",
      "Val Epoch Loss: 0.013551452799124251\n",
      "Training Epoch : [756/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011628606368934638\n",
      "Val Epoch Loss: 0.012371973823112632\n",
      "Training Epoch : [757/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011243623093162713\n",
      "Val Epoch Loss: 0.016837684057750983\n",
      "Training Epoch : [758/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012791253234210768\n",
      "Val Epoch Loss: 0.0159889603842442\n",
      "Training Epoch : [759/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011339284038465274\n",
      "Val Epoch Loss: 0.011678220423116153\n",
      "Training Epoch : [760/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01122537919467217\n",
      "Val Epoch Loss: 0.013438229976025852\n",
      "Training Epoch : [761/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012164985712029432\n",
      "Val Epoch Loss: 0.013818015614034314\n",
      "Training Epoch : [762/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011903657136779083\n",
      "Val Epoch Loss: 0.012644605616132091\n",
      "Training Epoch : [763/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01144555370372377\n",
      "Val Epoch Loss: 0.013158222387765386\n",
      "Training Epoch : [764/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011279482173880464\n",
      "Val Epoch Loss: 0.01334227088955231\n",
      "Training Epoch : [765/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013344056389637683\n",
      "Val Epoch Loss: 0.012602679923140002\n",
      "Training Epoch : [766/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011354166058529364\n",
      "Val Epoch Loss: 0.012379645923685626\n",
      "Training Epoch : [767/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011402343301788756\n",
      "Val Epoch Loss: 0.0157164713792123\n",
      "Training Epoch : [768/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012212173008408985\n",
      "Val Epoch Loss: 0.012621560990258953\n",
      "Training Epoch : [769/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010949222419999148\n",
      "Val Epoch Loss: 0.012023535329621825\n",
      "Training Epoch : [770/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012198133320596657\n",
      "Val Epoch Loss: 0.014189325548757455\n",
      "Training Epoch : [771/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012214360513577336\n",
      "Val Epoch Loss: 0.012167882157403878\n",
      "Training Epoch : [772/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011307570409323824\n",
      "Val Epoch Loss: 0.013331019895543394\n",
      "Training Epoch : [773/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011763905714216986\n",
      "Val Epoch Loss: 0.013481507953225733\n",
      "Training Epoch : [774/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012560816530726458\n",
      "Val Epoch Loss: 0.012000836470274646\n",
      "Training Epoch : [775/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011982735981674571\n",
      "Val Epoch Loss: 0.0172655293748616\n",
      "Training Epoch : [776/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011903736495265835\n",
      "Val Epoch Loss: 0.013388086421304922\n",
      "Training Epoch : [777/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011647494889697746\n",
      "Val Epoch Loss: 0.014893879205613447\n",
      "Training Epoch : [778/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012159707857982108\n",
      "Val Epoch Loss: 0.012365494487120918\n",
      "Training Epoch : [779/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011898796417211232\n",
      "Val Epoch Loss: 0.01622150995535776\n",
      "Training Epoch : [780/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011970009728285828\n",
      "Val Epoch Loss: 0.01299418755929525\n",
      "Training Epoch : [781/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012414492789263787\n",
      "Val Epoch Loss: 0.012176579794246018\n",
      "Training Epoch : [782/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012329032889714367\n",
      "Val Epoch Loss: 0.011694619217642435\n",
      "Training Epoch : [783/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011856195504630082\n",
      "Val Epoch Loss: 0.011510301292506292\n",
      "Training Epoch : [784/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01166031847854978\n",
      "Val Epoch Loss: 0.01449660417979191\n",
      "Training Epoch : [785/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01144663308207926\n",
      "Val Epoch Loss: 0.01208530216765786\n",
      "Training Epoch : [786/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01171051872600066\n",
      "Val Epoch Loss: 0.012927234313744856\n",
      "Training Epoch : [787/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013229621770350556\n",
      "Val Epoch Loss: 0.014916461063732737\n",
      "Training Epoch : [788/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012505393083158293\n",
      "Val Epoch Loss: 0.013039647298252308\n",
      "Training Epoch : [789/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013567965600247445\n",
      "Val Epoch Loss: 0.014069909150222023\n",
      "Training Epoch : [790/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012916473157115672\n",
      "Val Epoch Loss: 0.012391493284284431\n",
      "Training Epoch : [791/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01153451302333882\n",
      "Val Epoch Loss: 0.01312942746230156\n",
      "Training Epoch : [792/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011557115045817275\n",
      "Val Epoch Loss: 0.011841841674667146\n",
      "Training Epoch : [793/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011440995762026623\n",
      "Val Epoch Loss: 0.01464942263938658\n",
      "Training Epoch : [794/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011530570832914427\n",
      "Val Epoch Loss: 0.013644900817828449\n",
      "Training Epoch : [795/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012462041022157982\n",
      "Val Epoch Loss: 0.01460146065801382\n",
      "Training Epoch : [796/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011409639331855272\n",
      "Val Epoch Loss: 0.012696413648020672\n",
      "Training Epoch : [797/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011704025079349154\n",
      "Val Epoch Loss: 0.015738485765484114\n",
      "Training Epoch : [798/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013275623174482271\n",
      "Val Epoch Loss: 0.012564469018558922\n",
      "Training Epoch : [799/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011948147077897662\n",
      "Val Epoch Loss: 0.013121519415443273\n",
      "Training Epoch : [800/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012078742031008005\n",
      "Val Epoch Loss: 0.012520515638722205\n",
      "Training Epoch : [801/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011557876622598422\n",
      "Val Epoch Loss: 0.012639135735011414\n",
      "Training Epoch : [802/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012226536943528214\n",
      "Val Epoch Loss: 0.013987949326340305\n",
      "Training Epoch : [803/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011867151154499305\n",
      "Val Epoch Loss: 0.012455468439791156\n",
      "Training Epoch : [804/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013385351208087645\n",
      "Val Epoch Loss: 0.01306937691620796\n",
      "Training Epoch : [805/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012529020271215\n",
      "Val Epoch Loss: 0.012759242312504762\n",
      "Training Epoch : [806/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011985702366617165\n",
      "Val Epoch Loss: 0.014155794492628621\n",
      "Training Epoch : [807/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013180756500284923\n",
      "Val Epoch Loss: 0.011889808504125349\n",
      "Training Epoch : [808/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011125030424936037\n",
      "Val Epoch Loss: 0.011565518207805181\n",
      "Training Epoch : [809/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011890904781849761\n",
      "Val Epoch Loss: 0.01139225039591302\n",
      "Training Epoch : [810/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01155321067199111\n",
      "Val Epoch Loss: 0.013640714705137438\n",
      "Training Epoch : [811/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011568025220185518\n",
      "Val Epoch Loss: 0.01186555582650104\n",
      "Training Epoch : [812/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011708344014263466\n",
      "Val Epoch Loss: 0.013267509737297109\n",
      "Training Epoch : [813/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01168822504481987\n",
      "Val Epoch Loss: 0.012125086468203287\n",
      "Training Epoch : [814/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011126964794177758\n",
      "Val Epoch Loss: 0.014067120168443867\n",
      "Training Epoch : [815/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012042053877130934\n",
      "Val Epoch Loss: 0.012539123243952514\n",
      "Training Epoch : [816/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011930043073861222\n",
      "Val Epoch Loss: 0.011561627058606399\n",
      "Training Epoch : [817/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011614608921502767\n",
      "Val Epoch Loss: 0.014948404124496798\n",
      "Training Epoch : [818/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012699679741145749\n",
      "Val Epoch Loss: 0.013682056463470585\n",
      "Training Epoch : [819/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011720320307894758\n",
      "Val Epoch Loss: 0.015756642539360512\n",
      "Training Epoch : [820/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01211765984465417\n",
      "Val Epoch Loss: 0.013056963630094143\n",
      "Training Epoch : [821/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011611969896445149\n",
      "Val Epoch Loss: 0.01684921956008398\n",
      "Training Epoch : [822/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012330778776422927\n",
      "Val Epoch Loss: 0.011542285486939363\n",
      "Training Epoch : [823/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011494259031391457\n",
      "Val Epoch Loss: 0.0137855614609036\n",
      "Training Epoch : [824/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01217651163767043\n",
      "Val Epoch Loss: 0.012235536197151401\n",
      "Training Epoch : [825/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011659760003615367\n",
      "Val Epoch Loss: 0.01363870202361508\n",
      "Training Epoch : [826/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01210903746419047\n",
      "Val Epoch Loss: 0.013976529314133682\n",
      "Training Epoch : [827/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012932214689882178\n",
      "Val Epoch Loss: 0.01278942525325539\n",
      "Training Epoch : [828/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013674580185723147\n",
      "Val Epoch Loss: 0.013415516213219809\n",
      "Training Epoch : [829/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012226246738512265\n",
      "Val Epoch Loss: 0.012251857694201661\n",
      "Training Epoch : [830/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011085410675916233\n",
      "Val Epoch Loss: 0.011563783735925282\n",
      "Training Epoch : [831/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011113599065299096\n",
      "Val Epoch Loss: 0.012375516623969338\n",
      "Training Epoch : [832/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011199326695580231\n",
      "Val Epoch Loss: 0.012648557194839477\n",
      "Training Epoch : [833/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011378724532397954\n",
      "Val Epoch Loss: 0.01408753824408019\n",
      "Training Epoch : [834/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012404154054820538\n",
      "Val Epoch Loss: 0.011345165223725704\n",
      "Training Epoch : [835/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01114907069131732\n",
      "Val Epoch Loss: 0.01214324556119544\n",
      "Training Epoch : [836/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010876685270647468\n",
      "Val Epoch Loss: 0.012365739037417561\n",
      "Training Epoch : [837/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011620950566506699\n",
      "Val Epoch Loss: 0.013248873004493745\n",
      "Training Epoch : [838/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011594542571784635\n",
      "Val Epoch Loss: 0.018589373779066495\n",
      "Training Epoch : [839/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013254954945296049\n",
      "Val Epoch Loss: 0.015399779815006218\n",
      "Training Epoch : [840/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011648359218318211\n",
      "Val Epoch Loss: 0.013730664726800137\n",
      "Training Epoch : [841/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011807159214329562\n",
      "Val Epoch Loss: 0.01193692501309622\n",
      "Training Epoch : [842/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011921701977323545\n",
      "Val Epoch Loss: 0.015224789793137461\n",
      "Training Epoch : [843/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013972764663202198\n",
      "Val Epoch Loss: 0.012244970732258241\n",
      "Training Epoch : [844/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01152929604837769\n",
      "Val Epoch Loss: 0.012749616957367644\n",
      "Training Epoch : [845/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011544214647361323\n",
      "Val Epoch Loss: 0.012485977574741762\n",
      "Training Epoch : [846/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01140177007274408\n",
      "Val Epoch Loss: 0.015508089012368336\n",
      "Training Epoch : [847/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013120697517144052\n",
      "Val Epoch Loss: 0.017554306357755865\n",
      "Training Epoch : [848/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01295549150458292\n",
      "Val Epoch Loss: 0.01259307529605729\n",
      "Training Epoch : [849/1000]===============================================================================\n",
      "Training Epoch Loss: 0.015006534681704483\n",
      "Val Epoch Loss: 0.015650006072399646\n",
      "Training Epoch : [850/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013120035787946299\n",
      "Val Epoch Loss: 0.013782423289835845\n",
      "Training Epoch : [851/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012651649499802213\n",
      "Val Epoch Loss: 0.012666108153337663\n",
      "Training Epoch : [852/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011826711608783194\n",
      "Val Epoch Loss: 0.012400463610997815\n",
      "Training Epoch : [853/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012719205114990473\n",
      "Val Epoch Loss: 0.012239041695322254\n",
      "Training Epoch : [854/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012675571206368898\n",
      "Val Epoch Loss: 0.012371243273312422\n",
      "Training Epoch : [855/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01196379743908581\n",
      "Val Epoch Loss: 0.01228626042154103\n",
      "Training Epoch : [856/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011368613775917575\n",
      "Val Epoch Loss: 0.012714907421277052\n",
      "Training Epoch : [857/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012029691158156646\n",
      "Val Epoch Loss: 0.012954038290588773\n",
      "Training Epoch : [858/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011962569649576357\n",
      "Val Epoch Loss: 0.014252331120109088\n",
      "Training Epoch : [859/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011673706583678722\n",
      "Val Epoch Loss: 0.012065073340444973\n",
      "Training Epoch : [860/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010947948689327427\n",
      "Val Epoch Loss: 0.012734824068550216\n",
      "Training Epoch : [861/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01169220912025163\n",
      "Val Epoch Loss: 0.012875165333848838\n",
      "Training Epoch : [862/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01144215915548174\n",
      "Val Epoch Loss: 0.012200380124053672\n",
      "Training Epoch : [863/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011710425422183778\n",
      "Val Epoch Loss: 0.012065941611868575\n",
      "Training Epoch : [864/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011117032704580771\n",
      "Val Epoch Loss: 0.01660695067288256\n",
      "Training Epoch : [865/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011769853409771857\n",
      "Val Epoch Loss: 0.012241647203489648\n",
      "Training Epoch : [866/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01142683910745147\n",
      "Val Epoch Loss: 0.013141793004347403\n",
      "Training Epoch : [867/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011247932004105104\n",
      "Val Epoch Loss: 0.01291141323561437\n",
      "Training Epoch : [868/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011266005850446067\n",
      "Val Epoch Loss: 0.013173722288277196\n",
      "Training Epoch : [869/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011098281553897419\n",
      "Val Epoch Loss: 0.011994256552976654\n",
      "Training Epoch : [870/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011540975078548255\n",
      "Val Epoch Loss: 0.012784104306391734\n",
      "Training Epoch : [871/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011968650340445731\n",
      "Val Epoch Loss: 0.012537851160710775\n",
      "Training Epoch : [872/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011420200456325946\n",
      "Val Epoch Loss: 0.012784965627361089\n",
      "Training Epoch : [873/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011010294473779044\n",
      "Val Epoch Loss: 0.011847571456057363\n",
      "Training Epoch : [874/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011792208055818551\n",
      "Val Epoch Loss: 0.01202248304923016\n",
      "Training Epoch : [875/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011434017288449564\n",
      "Val Epoch Loss: 0.013129893431476759\n",
      "Training Epoch : [876/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012301335674955657\n",
      "Val Epoch Loss: 0.011691276312615772\n",
      "Training Epoch : [877/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01092071703782207\n",
      "Val Epoch Loss: 0.012551456710513631\n",
      "Training Epoch : [878/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012286609259286993\n",
      "Val Epoch Loss: 0.012942644586713103\n",
      "Training Epoch : [879/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011729968233818286\n",
      "Val Epoch Loss: 0.013175809063556554\n",
      "Training Epoch : [880/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012393427718626825\n",
      "Val Epoch Loss: 0.014054247551125866\n",
      "Training Epoch : [881/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012783878727963096\n",
      "Val Epoch Loss: 0.013922632794435086\n",
      "Training Epoch : [882/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011262500854699235\n",
      "Val Epoch Loss: 0.012176188739862195\n",
      "Training Epoch : [883/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01218241232594377\n",
      "Val Epoch Loss: 0.0117961322853501\n",
      "Training Epoch : [884/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011090625408350638\n",
      "Val Epoch Loss: 0.013520698844283623\n",
      "Training Epoch : [885/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012292770301236919\n",
      "Val Epoch Loss: 0.013834814433817212\n",
      "Training Epoch : [886/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011433032512860862\n",
      "Val Epoch Loss: 0.01220279156328424\n",
      "Training Epoch : [887/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010556001301952883\n",
      "Val Epoch Loss: 0.012227201173357715\n",
      "Training Epoch : [888/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01169107226948989\n",
      "Val Epoch Loss: 0.011728654779481554\n",
      "Training Epoch : [889/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012192377918644956\n",
      "Val Epoch Loss: 0.012644271922334539\n",
      "Training Epoch : [890/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010942795387420216\n",
      "Val Epoch Loss: 0.01256450572614803\n",
      "Training Epoch : [891/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01253974788185013\n",
      "Val Epoch Loss: 0.017556837533208493\n",
      "Training Epoch : [892/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01619521246634816\n",
      "Val Epoch Loss: 0.017551174019708446\n",
      "Training Epoch : [893/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014695805938620316\n",
      "Val Epoch Loss: 0.014088104529563631\n",
      "Training Epoch : [894/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013477336174171222\n",
      "Val Epoch Loss: 0.012553711513694572\n",
      "Training Epoch : [895/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012089781144535855\n",
      "Val Epoch Loss: 0.013153201211121325\n",
      "Training Epoch : [896/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01291368474056454\n",
      "Val Epoch Loss: 0.012325386781747895\n",
      "Training Epoch : [897/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011214378632997212\n",
      "Val Epoch Loss: 0.012913661724795881\n",
      "Training Epoch : [898/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011839079057895824\n",
      "Val Epoch Loss: 0.013110534775095354\n",
      "Training Epoch : [899/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012723950399576049\n",
      "Val Epoch Loss: 0.011978250454902943\n",
      "Training Epoch : [900/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011912686554224868\n",
      "Val Epoch Loss: 0.014038548470891424\n",
      "Training Epoch : [901/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010955143964996463\n",
      "Val Epoch Loss: 0.013536456938661439\n",
      "Training Epoch : [902/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010712752645639213\n",
      "Val Epoch Loss: 0.011403500568887562\n",
      "Training Epoch : [903/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011173898207121774\n",
      "Val Epoch Loss: 0.013550357617388823\n",
      "Training Epoch : [904/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010715252450226169\n",
      "Val Epoch Loss: 0.014647391033481415\n",
      "Training Epoch : [905/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011371748448398552\n",
      "Val Epoch Loss: 0.012531466201548219\n",
      "Training Epoch : [906/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01122341682448199\n",
      "Val Epoch Loss: 0.013295776895431214\n",
      "Training Epoch : [907/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012469041675917412\n",
      "Val Epoch Loss: 0.013053418037567386\n",
      "Training Epoch : [908/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010889411229934347\n",
      "Val Epoch Loss: 0.012944897959211008\n",
      "Training Epoch : [909/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010926939069146388\n",
      "Val Epoch Loss: 0.012870662554632872\n",
      "Training Epoch : [910/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010907614934503249\n",
      "Val Epoch Loss: 0.012318944330265941\n",
      "Training Epoch : [911/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011083698586413735\n",
      "Val Epoch Loss: 0.011999833575253816\n",
      "Training Epoch : [912/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010638675382850986\n",
      "Val Epoch Loss: 0.011859381770543558\n",
      "Training Epoch : [913/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01069832866472241\n",
      "Val Epoch Loss: 0.01228421069160839\n",
      "Training Epoch : [914/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011338473481469248\n",
      "Val Epoch Loss: 0.012648563179506087\n",
      "Training Epoch : [915/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011155653377308658\n",
      "Val Epoch Loss: 0.014016344784881528\n",
      "Training Epoch : [916/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011093486039164034\n",
      "Val Epoch Loss: 0.012289462889863276\n",
      "Training Epoch : [917/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011270240659972555\n",
      "Val Epoch Loss: 0.012286335530102645\n",
      "Training Epoch : [918/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010869594891310522\n",
      "Val Epoch Loss: 0.012875216475406074\n",
      "Training Epoch : [919/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011707835683697149\n",
      "Val Epoch Loss: 0.0133638163158474\n",
      "Training Epoch : [920/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011484839120193532\n",
      "Val Epoch Loss: 0.011459014825767985\n",
      "Training Epoch : [921/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011282677517125481\n",
      "Val Epoch Loss: 0.013887182297031885\n",
      "Training Epoch : [922/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01116546446849641\n",
      "Val Epoch Loss: 0.012756918857217227\n",
      "Training Epoch : [923/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011460497656739071\n",
      "Val Epoch Loss: 0.012744931637395271\n",
      "Training Epoch : [924/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011654038423378216\n",
      "Val Epoch Loss: 0.013488838173400023\n",
      "Training Epoch : [925/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011519505470795067\n",
      "Val Epoch Loss: 0.012089388232868115\n",
      "Training Epoch : [926/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011872004227418648\n",
      "Val Epoch Loss: 0.014417100667120203\n",
      "Training Epoch : [927/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01244735791298904\n",
      "Val Epoch Loss: 0.013404723411536236\n",
      "Training Epoch : [928/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012046834723533769\n",
      "Val Epoch Loss: 0.013901282947412446\n",
      "Training Epoch : [929/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012472351939466438\n",
      "Val Epoch Loss: 0.011435925139123452\n",
      "Training Epoch : [930/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011896496761198106\n",
      "Val Epoch Loss: 0.013584392302283576\n",
      "Training Epoch : [931/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010933584337563891\n",
      "Val Epoch Loss: 0.01217240284772982\n",
      "Training Epoch : [932/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011415497497900537\n",
      "Val Epoch Loss: 0.013666042373331851\n",
      "Training Epoch : [933/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011117957213795498\n",
      "Val Epoch Loss: 0.011957778566284105\n",
      "Training Epoch : [934/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011308721658822737\n",
      "Val Epoch Loss: 0.01393544449360649\n",
      "Training Epoch : [935/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013184410691457359\n",
      "Val Epoch Loss: 0.014451890385795483\n",
      "Training Epoch : [936/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01163550282485391\n",
      "Val Epoch Loss: 0.012356627190265021\n",
      "Training Epoch : [937/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010809878279504023\n",
      "Val Epoch Loss: 0.013760692014138362\n",
      "Training Epoch : [938/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011193676238977596\n",
      "Val Epoch Loss: 0.012039722848720359\n",
      "Training Epoch : [939/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010974320877147349\n",
      "Val Epoch Loss: 0.01352654466264587\n",
      "Training Epoch : [940/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011025526548588746\n",
      "Val Epoch Loss: 0.01155696732396456\n",
      "Training Epoch : [941/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011066074630147532\n",
      "Val Epoch Loss: 0.011968540382344815\n",
      "Training Epoch : [942/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011937765747700868\n",
      "Val Epoch Loss: 0.011868306267148765\n",
      "Training Epoch : [943/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010944228741879525\n",
      "Val Epoch Loss: 0.011897412410594131\n",
      "Training Epoch : [944/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010776940635160395\n",
      "Val Epoch Loss: 0.011637726427196819\n",
      "Training Epoch : [945/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011113214306533337\n",
      "Val Epoch Loss: 0.012016768133770185\n",
      "Training Epoch : [946/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011196449448011424\n",
      "Val Epoch Loss: 0.015207091671120571\n",
      "Training Epoch : [947/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011549970527228556\n",
      "Val Epoch Loss: 0.01218416082280639\n",
      "Training Epoch : [948/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010788218387843747\n",
      "Val Epoch Loss: 0.01267922187674374\n",
      "Training Epoch : [949/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011072419877899321\n",
      "Val Epoch Loss: 0.013383417113034642\n",
      "Training Epoch : [950/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010724837695689578\n",
      "Val Epoch Loss: 0.012298372774047925\n",
      "Training Epoch : [951/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010496487007721475\n",
      "Val Epoch Loss: 0.01243428677308226\n",
      "Training Epoch : [952/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012291086396496547\n",
      "Val Epoch Loss: 0.013605564477314291\n",
      "Training Epoch : [953/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012029616725876144\n",
      "Val Epoch Loss: 0.012517357690835763\n",
      "Training Epoch : [954/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011369450900115464\n",
      "Val Epoch Loss: 0.012636073862315817\n",
      "Training Epoch : [955/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01128440536558628\n",
      "Val Epoch Loss: 0.013461097099168814\n",
      "Training Epoch : [956/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011726134615999303\n",
      "Val Epoch Loss: 0.01322188990429583\n",
      "Training Epoch : [957/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010848001454417644\n",
      "Val Epoch Loss: 0.011925324721041283\n",
      "Training Epoch : [958/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01128673386809073\n",
      "Val Epoch Loss: 0.012428283758804594\n",
      "Training Epoch : [959/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011356640460067674\n",
      "Val Epoch Loss: 0.01274392490373238\n",
      "Training Epoch : [960/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011360294712511333\n",
      "Val Epoch Loss: 0.012724366792673735\n",
      "Training Epoch : [961/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01242479086412411\n",
      "Val Epoch Loss: 0.015266717031696126\n",
      "Training Epoch : [962/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011992709221024262\n",
      "Val Epoch Loss: 0.0134213359913127\n",
      "Training Epoch : [963/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012133562422700618\n",
      "Val Epoch Loss: 0.0156950289904336\n",
      "Training Epoch : [964/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011255874408801136\n",
      "Val Epoch Loss: 0.013296670695509468\n",
      "Training Epoch : [965/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011292775455666216\n",
      "Val Epoch Loss: 0.012002759326653751\n",
      "Training Epoch : [966/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010711994426521031\n",
      "Val Epoch Loss: 0.0114700658105951\n",
      "Training Epoch : [967/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011030767125224597\n",
      "Val Epoch Loss: 0.0118464768518032\n",
      "Training Epoch : [968/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010985336597322634\n",
      "Val Epoch Loss: 0.012424566890235599\n",
      "Training Epoch : [969/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012160242314597494\n",
      "Val Epoch Loss: 0.013533402945062048\n",
      "Training Epoch : [970/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011912752702636155\n",
      "Val Epoch Loss: 0.012771600514258208\n",
      "Training Epoch : [971/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011126591137757427\n",
      "Val Epoch Loss: 0.0127376365095475\n",
      "Training Epoch : [972/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011644856734691482\n",
      "Val Epoch Loss: 0.011969419440567052\n",
      "Training Epoch : [973/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010743287257163933\n",
      "Val Epoch Loss: 0.013563781823521774\n",
      "Training Epoch : [974/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010627182482398655\n",
      "Val Epoch Loss: 0.011975030048664561\n",
      "Training Epoch : [975/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011449687673073066\n",
      "Val Epoch Loss: 0.01223701928239806\n",
      "Training Epoch : [976/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011551013363427237\n",
      "Val Epoch Loss: 0.014034222921748695\n",
      "Training Epoch : [977/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010993589818673698\n",
      "Val Epoch Loss: 0.011595978086820412\n",
      "Training Epoch : [978/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011275866975713717\n",
      "Val Epoch Loss: 0.012572183463673451\n",
      "Training Epoch : [979/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011703257620530693\n",
      "Val Epoch Loss: 0.011707511628628708\n",
      "Training Epoch : [980/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01145982132048199\n",
      "Val Epoch Loss: 0.013345061707935346\n",
      "Training Epoch : [981/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011236220696254781\n",
      "Val Epoch Loss: 0.012390439015641613\n",
      "Training Epoch : [982/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011028946303811512\n",
      "Val Epoch Loss: 0.013967728138117022\n",
      "Training Epoch : [983/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011638798187241742\n",
      "Val Epoch Loss: 0.013565305993037518\n",
      "Training Epoch : [984/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011318347233004476\n",
      "Val Epoch Loss: 0.01322482009632758\n",
      "Training Epoch : [985/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01200741071155981\n",
      "Val Epoch Loss: 0.012786088048750045\n",
      "Training Epoch : [986/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011523531480251174\n",
      "Val Epoch Loss: 0.01668110061559434\n",
      "Training Epoch : [987/1000]===============================================================================\n",
      "Training Epoch Loss: 0.014961871563604004\n",
      "Val Epoch Loss: 0.01225887626826175\n",
      "Training Epoch : [988/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012260302925776494\n",
      "Val Epoch Loss: 0.013565606642882094\n",
      "Training Epoch : [989/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010961749639950301\n",
      "Val Epoch Loss: 0.012180879535950035\n",
      "Training Epoch : [990/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01075556711293757\n",
      "Val Epoch Loss: 0.012301573656977302\n",
      "Training Epoch : [991/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010858856374397874\n",
      "Val Epoch Loss: 0.012356706192612796\n",
      "Training Epoch : [992/1000]===============================================================================\n",
      "Training Epoch Loss: 0.010810904021031763\n",
      "Val Epoch Loss: 0.013100085856613555\n",
      "Training Epoch : [993/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011620297023144207\n",
      "Val Epoch Loss: 0.012921152820988362\n",
      "Training Epoch : [994/1000]===============================================================================\n",
      "Training Epoch Loss: 0.01054822555498073\n",
      "Val Epoch Loss: 0.01227617644288234\n",
      "Training Epoch : [995/1000]===============================================================================\n",
      "Training Epoch Loss: 0.013336202448331997\n",
      "Val Epoch Loss: 0.013665385577999251\n",
      "Training Epoch : [996/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011410134795464967\n",
      "Val Epoch Loss: 0.013373676655311627\n",
      "Training Epoch : [997/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012304613268689105\n",
      "Val Epoch Loss: 0.01347587896682518\n",
      "Training Epoch : [998/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012291665385036092\n",
      "Val Epoch Loss: 0.01887806784361601\n",
      "Training Epoch : [999/1000]===============================================================================\n",
      "Training Epoch Loss: 0.012645975318982414\n",
      "Val Epoch Loss: 0.012422477569136964\n",
      "Training Epoch : [1000/1000]===============================================================================\n",
      "Training Epoch Loss: 0.011319459097361878\n",
      "Val Epoch Loss: 0.015033942002398697\n"
     ]
    }
   ],
   "source": [
    "val_best_loss = 10000000\n",
    "\n",
    "for epoch_idx in range(n_epochs):\n",
    "    print(f\"Training Epoch : [{epoch_idx+1}/{n_epochs}]===============================================================================\")\n",
    "    \n",
    "    train(encoder,decoder,Q,opt,train_data_loader,is_wandb,verbose_freq = 500)    \n",
    "    val_losses = validate(encoder,decoder,Q,val_data_loader,is_wandb) \n",
    "    \n",
    "    if (val_losses < val_best_loss) and is_wandb: \n",
    "        val_best_loss = val_losses\n",
    "        print(\"Saving Best Model =======================================>\")\n",
    "        torch.save(encoder, f'{exp_path}/encoder.pth.tar')\n",
    "        torch.save(decoder, f'{exp_path}/decoder.pth.tar')\n",
    "        torch.save(Q, f'{exp_path}/Q.pth.tar')\n",
    "        wandb.log({\"epoch_saved_Best_Model\": epoch_idx+1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a24dc9-141d-4786-8ab3-fef432bc120e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
