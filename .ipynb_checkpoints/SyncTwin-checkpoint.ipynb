{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "99ddaf2a-fe69-4785-b9a0-9c4232abf07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Trying to reproduce the results using transformers\n",
    "\n",
    "import copy\n",
    "from typing import Optional, Any\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Module\n",
    "from torch.nn import MultiheadAttention\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import Linear\n",
    "from torch.nn import LayerNorm, BatchNorm1d\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from config.config import DEVICE\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6e1b9-7a69-4b11-9af9-e2e00eef8a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d88fb9f-59d2-4a4c-bda8-9f71c67137a2",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7494cc5-1591-4d75-85c2-bb7df3746797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_config(data_path, fold=\"train\"):\n",
    "    with open(data_path.format(fold, \"config\", \"pkl\"), \"rb\") as f:\n",
    "        config = pickle.load(file=f)\n",
    "    n_units = config[\"n_units\"]\n",
    "    n_treated = config[\"n_treated\"]\n",
    "    n_units_total = config[\"n_units_total\"]\n",
    "    step = config[\"step\"]\n",
    "    train_step = config[\"train_step\"]\n",
    "    control_sample = config[\"control_sample\"]\n",
    "    noise = config[\"noise\"]\n",
    "    n_basis = config[\"n_basis\"]\n",
    "    n_cluster = config[\"n_cluster\"]\n",
    "    return n_units, n_treated, n_units_total, step, train_step, control_sample, noise, n_basis, n_cluster\n",
    "\n",
    "def load_tensor(data_path, fold=\"train\"):\n",
    "    print(data_path.format(fold, \"x_full\", \"pth\"))\n",
    "    x_full = torch.load(data_path.format(fold, \"x_full\", \"pth\"))\n",
    "    t_full = torch.load(data_path.format(fold, \"t_full\", \"pth\"))\n",
    "    mask_full = torch.load(data_path.format(fold, \"mask_full\", \"pth\"))\n",
    "    batch_ind_full = torch.load(data_path.format(fold, \"batch_ind_full\", \"pth\"))\n",
    "    y_full = torch.load(data_path.format(fold, \"y_full\", \"pth\"))\n",
    "    y_control = torch.load(data_path.format(fold, \"y_control\", \"pth\"))\n",
    "    y_mask_full = torch.load(data_path.format(fold, \"y_mask_full\", \"pth\"))\n",
    "    m = torch.load(data_path.format(fold, \"m\", \"pth\"))\n",
    "    sd = torch.load(data_path.format(fold, \"sd\", \"pth\"))\n",
    "    treatment_effect = torch.load(data_path.format(fold, \"treatment_effect\", \"pth\"))\n",
    "    return x_full, t_full, mask_full, batch_ind_full, y_full, y_control, y_mask_full, m, sd, treatment_effect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63dec31e-0d49-4782-97aa-ea97a08ea7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/sync6d-p10-seed-100\"+ \"/{}-{}.{}\"\n",
    "n_units, n_treated, n_units_total, step, train_step, control_sample, noise, n_basis, n_cluster = load_config(\n",
    "    data_path, \"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eb9fa60-bac1-4a0f-9ba0-44938f101e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 200 1200 30 25 1000 0.1 6 2\n"
     ]
    }
   ],
   "source": [
    "print(n_units, n_treated, n_units_total, step, train_step, control_sample, noise, n_basis, n_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a5eca5a-4f13-430d-a400-2bd71479c0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/sync6d-p10-seed-100/train-x_full.pth\n"
     ]
    }
   ],
   "source": [
    "(x_full,t_full,mask_full,batch_ind_full,y_full,y_control,y_mask_full,m,sd,treatment_effect,) = load_tensor(data_path, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483279f3-d5a6-4274-a50f-b074e73d1c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_full: torch.Size([25, 1200, 3])\n",
      "t_full: torch.Size([25, 1200, 3])\n",
      "mask_full: torch.Size([25, 1200, 3])\n",
      "batch_ind_full: torch.Size([1200])\n",
      "y_full: torch.Size([5, 1200, 1])\n",
      "y_control: torch.Size([5, 1000, 1])\n",
      "treatment_effect: torch.Size([5, 200, 1])\n",
      "y_mask_full: torch.Size([1200])\n",
      "m: torch.Size([3]) tensor([0.0118, 0.1373, 1.0291], device='cuda:3')\n",
      "sd: torch.Size([3]) tensor([0.7947, 3.5020, 7.6243], device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "print(f'x_full: {x_full.shape}') ### Temporal Covariates\n",
    "print(f't_full: {t_full.shape}') ###  Time -25 to 4\n",
    "print(f'mask_full: {mask_full.shape}') ### Masking vector\n",
    "print(f'batch_ind_full: {batch_ind_full.shape}') ### Batch indexes\n",
    "print(f'y_full: {y_full.shape}')   ### y_i ### need to predict this\n",
    "print(f'y_control: {y_control.shape}') #### y_i(0)\n",
    "print(f'treatment_effect: {treatment_effect.shape}')  #### y_i(1)\n",
    "print(f'y_mask_full: {y_mask_full.shape}') ### To separate control and treatment groups  \n",
    "print(f'm: {m.shape} {m}') \n",
    "print(f'sd: {sd.shape} {sd}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c8d5a04-7a0c-4b20-a391-21ddfebc984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDL_Stim_Dataset(Dataset):\n",
    "    def __init__(self, data_path, fold,device):\n",
    "        # Get the data\n",
    "        (self.x_full,self.t_full,self.mask_full,self.batch_ind_full,\n",
    "         self.y_full,self.y_control,self.y_mask_full,\n",
    "         self.m,self.sd,self.treatment_effect,) = load_tensor(data_path, fold)\n",
    "        print(self.m,self.sd)\n",
    "        # print(self.x_full.max(),self.x_full.min(),self.x_full.mean())\n",
    "        self.x_full = torch.moveaxis(self.x_full,1,0)\n",
    "        # self.x_full = torch.moveaxis(self.x_full,1,-1)\n",
    "        self.t_full = torch.moveaxis(self.t_full,1,0)\n",
    "        # self.t_full = torch.moveaxis(self.t_full,1,-1)\n",
    "        self.mask_full = torch.moveaxis(self.mask_full,1,0)\n",
    "        # self.mask_full = torch.moveaxis(self.mask_full,1,-1)\n",
    "        self.y_full = torch.moveaxis(self.y_full,1,0).squeeze()\n",
    "        self.y_control = torch.moveaxis(self.y_control,1,0)\n",
    "        self.treatment_effect = torch.moveaxis(self.treatment_effect,1,0)\n",
    "        # print(self.batch_ind_full)\n",
    "        self.device = device\n",
    "        # for i in range (self.x_full.shape[-1]):\n",
    "        #     self.x_full[:,:,i] = (self.x_full[:,:,i] - self.m[i])/self.sd[i]\n",
    "        # print(self.x_full.max(),self.x_full.min(),self.x_full.mean())\n",
    "        print(f'x_full: {self.x_full.shape}') ### Temporal Covariates\n",
    "        print(f't_full: {self.t_full.shape}') ###  Time -25 to 4\n",
    "        print(f'mask_full: {self.mask_full.shape}') ### Masking vector\n",
    "        print(f'batch_ind_full: {self.batch_ind_full.shape}') ### Batch indexes\n",
    "        print(f'y_full: {self.y_full.shape}')   ### y_i ### need to predict this\n",
    "        print(f'y_control: {self.y_control.shape}') #### y_i(0)\n",
    "        print(f'treatment_effect: {self.treatment_effect.shape}')  #### y_i(1)\n",
    "        print(f'y_mask_full: {self.y_mask_full.shape}') ### if outcome not available during \n",
    "        print(f'm: {self.m.shape}') \n",
    "        print(f'sd: {self.sd.shape}')\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_full)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x_full[idx].to(self.device)    \n",
    "        t = self.t_full[idx].to(self.device)    \n",
    "        m = self.mask_full[idx].to(self.device)    \n",
    "        y = self.y_full[idx].to(self.device)    \n",
    "        y_mask = self.y_mask_full[idx].unsqueeze(-1).to(self.device)    \n",
    "        return x,t,m,y,y_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdd52b89-d1d2-4e61-a9a7-0eb845ed7419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/sync6d-p10-seed-100/train-x_full.pth\n",
      "tensor([0.0118, 0.1373, 1.0291], device='cuda:3') tensor([0.7947, 3.5020, 7.6243], device='cuda:3')\n",
      "x_full: torch.Size([1200, 25, 3])\n",
      "t_full: torch.Size([1200, 25, 3])\n",
      "mask_full: torch.Size([1200, 25, 3])\n",
      "batch_ind_full: torch.Size([1200])\n",
      "y_full: torch.Size([1200, 5])\n",
      "y_control: torch.Size([1000, 5, 1])\n",
      "treatment_effect: torch.Size([200, 5, 1])\n",
      "y_mask_full: torch.Size([1200])\n",
      "m: torch.Size([3])\n",
      "sd: torch.Size([3])\n",
      "./data/sync6d-p10-seed-100/val-x_full.pth\n",
      "tensor([0.0024, 0.0946, 0.9151], device='cuda:3') tensor([0.7631, 3.3575, 7.2832], device='cuda:3')\n",
      "x_full: torch.Size([1200, 25, 3])\n",
      "t_full: torch.Size([1200, 25, 3])\n",
      "mask_full: torch.Size([1200, 25, 3])\n",
      "batch_ind_full: torch.Size([1200])\n",
      "y_full: torch.Size([1200, 5])\n",
      "y_control: torch.Size([1000, 5, 1])\n",
      "treatment_effect: torch.Size([200, 5, 1])\n",
      "y_mask_full: torch.Size([1200])\n",
      "m: torch.Size([3])\n",
      "sd: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./data/sync6d-p10-seed-100\"+ \"/{}-{}.{}\"\n",
    "train_dataset = LDL_Stim_Dataset(data_path, fold = 'train',device = DEVICE)\n",
    "val_dataset = LDL_Stim_Dataset(data_path, fold = 'val',device = DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a5c700e-b477-4817-a40b-79d4ca443010",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a41e1ef0-4e3d-4f73-9d7f-b696b6d8cc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 25, 3]) torch.Size([32, 25, 3]) torch.Size([32, 25, 3]) torch.Size([32, 5]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "x,t,m,y,y_mask   = next(iter(train_data_loader))\n",
    "print(x.shape,t.shape,m.shape,y.shape,y_mask.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4257d08-85f3-4f72-86dd-6a3fb78116c0",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70b5ba82-c99d-41d1-b8ab-3ed1f404bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "\n",
    "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5330de0-83e6-4c5e-93a0-8ae4f98e473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_Embedding(nn.Module): \n",
    "    def __init__(self, in_channels: int = 3, emb_size: int = 64):\n",
    "        super(get_Embedding, self).__init__()\n",
    "\n",
    "        self.projection =  nn.Sequential(\n",
    "            Rearrange('b s e -> b e s'),\n",
    "            nn.Conv1d(in_channels, emb_size, kernel_size = 1, stride = 1),\n",
    "            # Rearrange('b e s -> b s e')\n",
    "            )\n",
    "            \n",
    "        self.arrange1 = Rearrange('b e s -> s b e')\n",
    "        self.pos = PositionalEncoding(d_model=emb_size)\n",
    "        self.arrange2 = Rearrange('s b e -> b s e  ')\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.projection(x)  \n",
    "        # add position embedding\n",
    "        x = self.arrange1(x)\n",
    "        x = self.pos(x)\n",
    "        x = self.arrange2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d396614-3ab8-4c47-8faa-44469a50f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atten_block(nn.Module): \n",
    "    def __init__(self, d_model=64, nhead=8, dropout=0.1,dim_feedforward=512,\n",
    "                 layer_norm_eps=1e-5):\n",
    "        super(Atten_block, self).__init__()\n",
    "       \n",
    "        self.norm = LayerNorm(d_model, eps=layer_norm_eps)#, **factory_kwargs)  \n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)#,\n",
    "                                            # **factory_kwargs)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "        \n",
    "        self.norm_ff = LayerNorm(d_model, eps=layer_norm_eps)#, **factory_kwargs)\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)#, **factory_kwargs)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)#, **factory_kwargs)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    " \n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        src = x\n",
    "        src2 = self.self_attn(src, src, src)[0]\n",
    "        out = src + self.dropout(src2)\n",
    "        out = self.norm(out)   ########\n",
    "        \n",
    "        src2 = self.linear2(self.dropout1(self.relu(self.linear1(out))))\n",
    "        out = out + self.dropout2(src2)\n",
    "        out = self.norm_ff(out)\n",
    "        return out                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "285a8595-f745-42c3-9728-c669b54305ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trans_Encoder(nn.Module): \n",
    "    def __init__(self, in_channels = 3,d_model=64, nhead=8, dropout=0.1,dim_feedforward=512,\n",
    "                 layer_norm_eps=1e-5):\n",
    "        super(Trans_Encoder, self).__init__()\n",
    "        \n",
    "        self.embed = get_Embedding(in_channels = in_channels, emb_size = d_model)\n",
    "        self.atten_1 =  Atten_block(d_model=d_model, nhead=nhead, dropout=dropout,\n",
    "                                    dim_feedforward=dim_feedforward,layer_norm_eps=layer_norm_eps)\n",
    "        self.atten_2 =  Atten_block(d_model=d_model, nhead=nhead, dropout=dropout,\n",
    "                                    dim_feedforward=dim_feedforward,layer_norm_eps=layer_norm_eps)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.atten_1(x)\n",
    "        x = self.atten_2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class Trans_Decoder(nn.Module): \n",
    "    def __init__(self,out_channels= 3, d_model=64, nhead=8, dropout=0.1,dim_feedforward=512,\n",
    "                 layer_norm_eps=1e-5):\n",
    "        super(Trans_Decoder, self).__init__()\n",
    "        \n",
    "        # self.embed = get_Embedding(in_channels = 3, emb_size = d_model)\n",
    "        self.atten_1 =  Atten_block(d_model=d_model, nhead=nhead, dropout=dropout,\n",
    "                                    dim_feedforward=dim_feedforward,layer_norm_eps=layer_norm_eps)\n",
    "        self.atten_2 =  Atten_block(d_model=d_model, nhead=nhead, dropout=dropout,\n",
    "                                    dim_feedforward=dim_feedforward,layer_norm_eps=layer_norm_eps)\n",
    "        self.final = Linear(d_model, out_channels)\n",
    "        # self.arrange1 = Rearrange('b e s -> s b e')\n",
    "    def forward(self, c):\n",
    "        # x = self.embed(x)\n",
    "        x = self.atten_1(c)\n",
    "        x = self.atten_2(x)\n",
    "        x = self.final(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "\n",
    "class linear_cls(nn.Module):\n",
    "    def __init__(self,y_times = 5):\n",
    "        super(linear_cls, self).__init__()\n",
    "        \n",
    "        self.Q = nn.Linear(1600, y_times)\n",
    "    def forward(self, c):\n",
    "        x = torch.flatten(c, start_dim=1, end_dim=- 1) \n",
    "        x = self.Q(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97e16223-b08f-4c46-aa79-267848d36746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 25, 3]) torch.Size([32, 25, 3]) torch.Size([32, 25, 3]) torch.Size([32, 5]) torch.Size([32, 1])\n",
      "torch.Size([32, 25, 64]) torch.Size([32, 25, 3]) torch.Size([32, 5])\n"
     ]
    }
   ],
   "source": [
    "encoder = Trans_Encoder().to(DEVICE)\n",
    "decoder = Trans_Decoder().to(DEVICE)\n",
    "Q = linear_cls().to(DEVICE)\n",
    "\n",
    "## Testing\n",
    "x,t,m,y,y_mask   = next(iter(train_data_loader))\n",
    "print(x.shape,t.shape,m.shape,y.shape,y_mask.shape )\n",
    "\n",
    "c_hat = encoder(x)\n",
    "x_hat = decoder(c_hat)\n",
    "y_hat = Q(c_hat)\n",
    "\n",
    "print(c_hat.shape,x_hat.shape,y_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679430db-a882-4ed3-89eb-e9ea62ed10a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Representation learning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a701babd-fb77-4e51-9882-a4462fbba530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training parameters\n",
    "criterion = nn.L1Loss()\n",
    "lr_u  = 0.001   \n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "n_epochs = 1000\n",
    "save_model_freq = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0aa742a-eb56-49ef-a46c-ca720dbd034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate optimizers\n",
    "opt =  torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters())+ list(Q.parameters()), lr=lr_u, betas=(beta_1, beta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3e35bb3-6230-40a9-af04-2527f339d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_loss(x_hat,x,m):\n",
    "    criterion = nn.MSELoss()\n",
    "    return criterion(x_hat*m,x*m)\n",
    "\n",
    "def sup_loss(y_hat,y,y_mask):\n",
    "    criterion = nn.MSELoss()\n",
    "    return criterion(y_hat*y_mask,y*y_mask)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count if self.count != 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc231d79-ffc2-43a4-8fe4-2b1b3493dd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_wandb = False\n",
    "if is_wandb:\n",
    "    import wandb\n",
    "    wandb.init(project=\"SyncTwin\", entity=\"jathurshan_0330\")\n",
    "    wandb.run.name = \"Initial_With_Transformers\"\n",
    "    wandb.run.save()\n",
    "    \n",
    "    exp_path = f\"/home/jupyter-jathurshan/SyncTwin_Results/{wandb.run.name}\"\n",
    "    if not os.path.exists(exp_path):\n",
    "        os.mkdir(exp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e11545a-b005-4646-884c-65dd09a45f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder,decoder,Q,opt,data_loader,is_wandb,verbose_freq = 500):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    Q.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    train_losses = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    for batch_idx, (x,t,m,y,y_mask ) in enumerate(data_loader): \n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        c_hat = encoder(x)\n",
    "        x_hat = decoder(c_hat)\n",
    "        y_hat = Q(c_hat)\n",
    "        \n",
    "        loss = recon_loss(x_hat,x,m) + sup_loss(y_hat,y,y_mask)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        train_losses.update(loss.data.item())\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if (batch_idx+1) % verbose_freq == 0:\n",
    "            msg = 'Epoch: [{0}/{3}][{1}/{2}]\\t' \\\n",
    "                  'Time {batch_time.val:.3f}s ({batch_time.avg:.3f}s)\\t' \\\n",
    "                  'Speed {speed:.1f} samples/s\\t' \\\n",
    "                  'Data {data_time.val:.3f}s ({data_time.avg:.3f}s)\\t' \\\n",
    "                  'Loss {train_loss.val:.5f} ({train_loss.avg:.5f})\\t'.format(\n",
    "                      epoch_idx+1, batch_idx,len(data_loader), n_epochs , batch_time=batch_time,\n",
    "                      speed=x.size(0)/batch_time.val,\n",
    "                      data_time=data_time, train_loss=train_losses)\n",
    "            print(msg)\n",
    "        \n",
    "        if is_wandb:\n",
    "            wandb.log({\"batch_loss\": loss.data.item()})\n",
    "    \n",
    "    \n",
    "    if is_wandb:\n",
    "            wandb.log({\"train_epoch_loss\": train_losses.avg})\n",
    "            wandb.log({\"training time/Iter\": batch_time.sum/len(data_loader)})\n",
    "    \n",
    "    \n",
    "    # print(f\"Evaluation   Epoch : {epoch_idx+1}  =====================>\")\n",
    "    print(f\"Training Epoch Loss: {train_losses.avg}\")#,Training Time/Epoch: {batch_time.sum},Training Time/Iter: {batch_time.sum/len(data_loader)}\")\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d8c56ca-c973-4290-a253-baa6caaf6502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(encoder,decoder,Q,data_loader,is_wandb):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    Q.eval()\n",
    "    \n",
    "    val_losses = AverageMeter()\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x,t,m,y,y_mask) in enumerate(data_loader): \n",
    "            c_hat = encoder(x)\n",
    "            x_hat = decoder(c_hat)\n",
    "            y_hat = Q(c_hat)\n",
    "\n",
    "            loss = recon_loss(x_hat,x,m) + sup_loss(y_hat,y,y_mask)\n",
    "            \n",
    "            val_losses.update(loss.data.item())\n",
    "    \n",
    "    if is_wandb:\n",
    "            wandb.log({\"val_epoch_loss\": val_losses.avg})\n",
    "    \n",
    "    \n",
    "    # print(f\"Evaluation   Epoch : {epoch_idx+1}  =====================>\")\n",
    "    print(f\"Val Epoch Loss: {val_losses.avg}\")\n",
    "    return val_losses.avg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6d9cada-09c6-4bb7-a24d-a376576ecc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_best_loss = 10000000\n",
    "\n",
    "for epoch_idx in range(n_epochs):\n",
    "    print(f\"Training Epoch : [{epoch_idx+1}/{n_epochs}]===============================================================================\")\n",
    "    \n",
    "    train(encoder,decoder,Q,opt,train_data_loader,is_wandb,verbose_freq = 500)    \n",
    "    val_losses = validate(encoder,decoder,Q,val_data_loader,is_wandb) \n",
    "    \n",
    "    if (val_losses < val_best_loss) and is_wandb: \n",
    "        val_best_loss = val_losses\n",
    "        print(\"Saving Best Model =======================================>\")\n",
    "        torch.save(encoder, f'{exp_path}/encoder.pth.tar')\n",
    "        torch.save(decoder, f'{exp_path}/decoder.pth.tar')\n",
    "        torch.save(Q, f'{exp_path}/Q.pth.tar')\n",
    "        wandb.log({\"epoch_saved_Best_Model\": epoch_idx+1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a24dc9-141d-4786-8ab3-fef432bc120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b103cc49-0420-4024-92a2-4d54a2714603",
   "metadata": {},
   "source": [
    "# Inference for predicting conterfactual of y(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d74b9faf-6dd0-455f-8032-f0928f786b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDL_Stim_Inference_Dataset(Dataset):\n",
    "    def __init__(self, data_path, fold,device,group = 'Treated'):\n",
    "        # Get the data\n",
    "        (self.x_full,self.t_full,self.mask_full,self.batch_ind_full,\n",
    "         self.y_full,self.y_control,self.y_mask_full,\n",
    "         self.m,self.sd,self.treatment_effect,) = load_tensor(data_path, fold)\n",
    "        \n",
    "        \n",
    "        self.x_full = torch.moveaxis(self.x_full,1,0)\n",
    "        self.t_full = torch.moveaxis(self.t_full,1,0)\n",
    "        self.mask_full = torch.moveaxis(self.mask_full,1,0)\n",
    "        self.y_full = torch.moveaxis(self.y_full,1,0).squeeze()\n",
    "        self.y_control = torch.moveaxis(self.y_control,1,0)\n",
    "        self.treatment_effect = torch.moveaxis(self.treatment_effect,1,0)\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        print(f\"Loading {group} Group\")\n",
    "        if group == 'Treated':\n",
    "            self.x_full = self.x_full[self.y_mask_full==0]\n",
    "            self.t_full = self.t_full[self.y_mask_full==0]\n",
    "            self.mask_full = self.mask_full[self.y_mask_full==0]\n",
    "            self.y_full = self.y_full[self.y_mask_full==0]\n",
    "            self.batch_ind_full = self.batch_ind_full[self.y_mask_full==0]\n",
    "        elif group == 'Control':\n",
    "            self.x_full = self.x_full[self.y_mask_full==1]\n",
    "            self.t_full = self.t_full[self.y_mask_full==1]\n",
    "            self.mask_full = self.mask_full[self.y_mask_full==1]\n",
    "            self.y_full = self.y_full[self.y_mask_full==1]\n",
    "            self.batch_ind_full = self.batch_ind_full[self.y_mask_full==1]\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f'x_full: {self.x_full.shape}') ### Temporal Covariates\n",
    "        print(f't_full: {self.t_full.shape}') ###  Time -25 to 4\n",
    "        print(f'mask_full: {self.mask_full.shape}') ### Masking vector\n",
    "        print(f'batch_ind_full: {self.batch_ind_full.shape}') ### Batch indexes\n",
    "        print(f'y_full: {self.y_full.shape}')   ### y_i ### need to predict this\n",
    "        print(f'y_control: {self.y_control.shape}') #### y_i(0)\n",
    "        print(f'treatment_effect: {self.treatment_effect.shape}')  #### y_i(1)\n",
    "        print(f'y_mask_full: {self.y_mask_full.shape}') ### if outcome not available during \n",
    "        print(f'm: {self.m.shape}') \n",
    "        print(f'sd: {self.sd.shape}')\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_full)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x_full[idx].to(self.device)    \n",
    "        t = self.t_full[idx].to(self.device)    \n",
    "        m = self.mask_full[idx].to(self.device)    \n",
    "        y = self.y_full[idx].to(self.device)    \n",
    "        y_mask = self.y_mask_full[idx].unsqueeze(-1).to(self.device)  \n",
    "        batch_ind = self.batch_ind_full[idx].to(self.device)  \n",
    "        return x,t,m,y,y_mask,batch_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40294ad1-e6d3-496a-9cc1-30b3decbfcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/sync6d-p10-seed-100/test-x_full.pth\n",
      "Loading Treated Group\n",
      "x_full: torch.Size([200, 25, 3])\n",
      "t_full: torch.Size([200, 25, 3])\n",
      "mask_full: torch.Size([200, 25, 3])\n",
      "batch_ind_full: torch.Size([200])\n",
      "y_full: torch.Size([200, 5])\n",
      "y_control: torch.Size([1000, 5, 1])\n",
      "treatment_effect: torch.Size([200, 5, 1])\n",
      "y_mask_full: torch.Size([1200])\n",
      "m: torch.Size([3])\n",
      "sd: torch.Size([3])\n",
      "./data/sync6d-p10-seed-100/test-x_full.pth\n",
      "Loading Control Group\n",
      "x_full: torch.Size([1000, 25, 3])\n",
      "t_full: torch.Size([1000, 25, 3])\n",
      "mask_full: torch.Size([1000, 25, 3])\n",
      "batch_ind_full: torch.Size([1000])\n",
      "y_full: torch.Size([1000, 5])\n",
      "y_control: torch.Size([1000, 5, 1])\n",
      "treatment_effect: torch.Size([200, 5, 1])\n",
      "y_mask_full: torch.Size([1200])\n",
      "m: torch.Size([3])\n",
      "sd: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "test_treated_dataset = LDL_Stim_Inference_Dataset(data_path, fold = 'test',device = DEVICE,group = 'Treated') \n",
    "test_control_dataset = LDL_Stim_Inference_Dataset(data_path, fold = 'test',device = DEVICE,group = 'Control') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f872b186-35e2-4af1-ad6c-9ecb297ea694",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_data_loader = DataLoader(test_treated_dataset, batch_size = 200, shuffle = False)\n",
    "control_data_loader = DataLoader(test_control_dataset, batch_size = 1000, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6dbabd5e-73da-48ec-bfac-564371d5533d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 25, 3]) torch.Size([1000, 25, 3]) torch.Size([1000, 25, 3]) torch.Size([1000, 5]) torch.Size([1000, 1]) torch.Size([1000]) 1 1\n"
     ]
    }
   ],
   "source": [
    "x,t,m,y,y_mask,batch_ind   = next(iter(control_data_loader))\n",
    "print(x.shape,t.shape,m.shape,y.shape,y_mask.shape,batch_ind.shape,len(control_data_loader),len(treatment_data_loader) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55e6bc54-9f68-4f48-b39e-27a83bb00ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_ind[1]#.detach().cpu().numpy()\n",
    "# x[batch_ind[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea05160a-8b9f-4c29-9381-ab556df30fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representation(encoder,data_loader):\n",
    "    \n",
    "    for batch_idx, (x,t,m,y,y_mask,batch_ind) in enumerate(data_loader): \n",
    "        c_hat = encoder(x)\n",
    "        if batch_idx ==0:\n",
    "            c = c_hat\n",
    "        else:\n",
    "            c = torch.cat((c,c_hat),dim = 0)\n",
    "    return c\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85031525-7257-4db2-84f7-6fff944a0512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166272\n"
     ]
    }
   ],
   "source": [
    "#Load encoder\n",
    "enc_path = '/home/jupyter-jathurshan/SyncTwin_Results/Initial_With_Transformers/encoder.pth.tar'\n",
    "encoder = torch.load(enc_path).to(DEVICE)\n",
    "encoder.eval()\n",
    "print(sum([np.prod(p.size()) for p in encoder.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1863f3ba-b560-431d-9828-e97d11f80cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 25, 64]) torch.Size([1000, 25, 64])\n"
     ]
    }
   ],
   "source": [
    "c_treat  = get_representation(encoder,treatment_data_loader).detach().to(DEVICE)\n",
    "c_control  = get_representation(encoder,control_data_loader).detach().to(DEVICE)\n",
    "\n",
    "print(c_treat.shape,c_control.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0ed008ae-b61d-435f-8600-ef1626343e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_loss(c_hat,c):\n",
    "    criterion = nn.MSELoss()\n",
    "    return criterion(c_hat,c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "4e8430a7-79eb-4b3c-a3d9-f5aa09c6c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_B(c_target,c_non_target,num_iterations=1000):\n",
    "    B = torch.zeros(len(c_target),len(c_non_target),1,1).to(DEVICE).requires_grad_(True)#nn.Parameter(torch.zeros(len(c_target),len(c_non_target),1,1)).to(DEVICE)\n",
    "    print(B.shape,c_target.shape,c_non_target.shape)\n",
    "    \n",
    "    plt.figure(figsize=(30,10))\n",
    "    im = plt.imshow(B.squeeze().detach().cpu().numpy(),cmap = 'jet')\n",
    "    plt.colorbar(shrink = 0.5)\n",
    "    plt.show()\n",
    "    \n",
    "    opt =  torch.optim.Adam([B], lr=0.001, betas=(0.9, 0.999))\n",
    "    matching_losses = []\n",
    "    for i in range(num_iterations):\n",
    "        opt.zero_grad()\n",
    "        target_ind = np.random.choice(len(c_target),32,replace=False)\n",
    "        c_target_sample = c_target[target_ind]\n",
    "        loss = 0\n",
    "        for ind in range(len(target_ind)):\n",
    "            c_est = B[target_ind[ind]]*c_non_target#F.gumbel_softmax(B[target_ind[ind]]*c_non_target, tau=1, hard=False)\n",
    "            \n",
    "            if ind ==0:\n",
    "                c_estimate = torch.sum(c_est,axis=0).unsqueeze(0)\n",
    "            else:\n",
    "                c_estimate = torch.cat((c_estimate,torch.sum(c_est,axis=0).unsqueeze(0)),dim=0)\n",
    "    \n",
    "        loss = matching_loss(c_estimate,c_target_sample)#/len(target_ind) \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        matching_losses.append(loss.data.item())\n",
    "        \n",
    "    # print(matching_losses)\n",
    "    plt.figure()\n",
    "    plt.plot(matching_losses)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(30,10))\n",
    "    plt.imshow(B.squeeze().detach().cpu().numpy(),cmap = 'jet')\n",
    "    plt.colorbar(shrink = 0.5)\n",
    "    plt.show()\n",
    "    \n",
    "    return B\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff90036-32c3-4779-ac90-de6bcbbceeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 1000, 1, 1]) torch.Size([200, 25, 64]) torch.Size([1000, 25, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "B_treat = learn_B(c_treat,c_control,num_iterations =10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6577244-191a-4e23-a8f3-6360a4dcc951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
